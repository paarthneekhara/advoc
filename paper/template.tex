\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2019}

\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}

%JULIAN: Is this conference double-blind?
\title{Expediting TTS Synthesis with Adversarial Vocoding}
\name{\text{*}Paarth Neekhara$^1$, \text{*}Chris Donahue$^{2}$, Miller Puckette$^2$, Shlomo Dubnov$^2$, Julian McAuley$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$UC San Diego Department of Computer Science\\
  $^2$UC San Diego Department of Music\\
  \text{*} Equal contribution}
\email{pneekhar@eng.ucsd.edu, cdonahue@ucsd.edu}

\begin{document}

\maketitle
% 
\begin{abstract}
Recent approaches in text-to-speech (TTS) synthesis employ neural network strategies to \emph{vocode} perceptually-informed spectrogram representations directly into listenable waveforms. 
Such vocoding procedures create a computational bottleneck in modern TTS pipelines. 
We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be heuristically vocoded. 
Through a user study, we show that our approach significantly outperforms na\"ive vocoding strategies while being hundreds of times faster than neural network vocoders used in state-of-the-art TTS systems. 
Because our approach is independent of the TTS pipeline, it has the potential to benefit numerous applications where fast vocoding of spectrograms is desirable. 
%JULIAN: This example sounds quite abstract. Say that it can be used for audio generation before saying the specifics, otherwise it's not clear what the significance of the example is.
%As an example, we show that our approach can be used to effectively vocode spectrograms generated by a separate GAN, improving the state of the art in unsupervised audio generation.
\end{abstract}

%\noindent\textbf{Index Terms}: text-to-speech, vocoding, generative adversarial networks

\section{Introduction}

Generating natural-sounding speech from text is a well-studied problem with numerous potential applications. 
While past approaches were built on extensive engineering knowledge in the areas of linguistics and speech processing  (see %JULIAN: Better to add a placeholder name here so that it actually generates an error. Easy to miss otherwise.
\cite{todo} for a review), 
recent approaches adopt neural network strategies which learn from data to map linguistic representations into audio waveforms~\cite{arik2017deep,gibiansky2017deep,ping2017deep,wang2017tacotron,shen2018natural}. 
Of these recent systems, 
the best performing~\cite{ping2017deep,shen2018natural} are both comprised of two functional and independent mechanisms which 
(1) map language into \emph{perceptually-informed spectrogram} representations (i.e.,~time-frequency decompositions of audio with logarithmic scaling of both frequency and amplitude), and 
(2) \emph{vocode} the resultant spectrograms into listenable waveforms. 
In this work, we focus our efforts on the vocoding subproblem.

%While lossy, TTS systems have been observed to benefit from operating in such domains.
%In~\cite{ping2017deep}, TTS systems pairing perceptual spectrogram generation with autoregressive vocoding were found to substantially outperform systems which operated in heuristically-invertible spectrogram domains.
%Hence, autoregressive vocoding is central to the success of state-of-the-art TTS systems.
%This finding underlines a need for faster strategies for vocoding perceptually-informed spectrograms to audio which do not compromise on audio quality.

While the aforementioned TTS approaches differ substantially in their methods of mapping language to spectrograms, they both depend on the same method to vocode spectrograms into high-fidelity, natural-sounding waveforms. 
Namely, they both utilize WaveNets~\cite{oord2016wavenet} conditioned on the synthetic spectrograms to synthesize audio. 
WaveNet is an autoregressive method which learns to predict individual audio samples given previous audio samples and---in this case---the desired spectrogram. 
Henceforth we will refer to this practice of using WaveNet to map spectrograms into waveforms as \emph{autoregressive vocoding}. 

Autoregressive vocoding is problematic as it necessitates running WaveNet once per individual audio sample (e.g. $22050$ times per second), bottlenecking the overall TTS system as the language-to-spectrogram mechanisms are comparatively fast.\footnote{In our empirical experimentation with open-source codebases, the autoregressive vocoding phase was over $1500$ times slower on average than the language to spectrogram phase.} 
Cumbersome
strategies like knowledge distillation~\cite{hinton2015distilling} and customized inference kernels~\cite{arik2017deep} can be used to accelerate WaveNet inference to be faster than real-time. 
However, 
we argue that WaveNet is fundamentally being tasked with solving two problems at once, 
while more efficient solutions to only one of these problems may suffice.
%and that a solution to only one of these problems may suffice and ultimately be more efficient.

%the usage of WaveNet is fundamentally trying to solve two problems at once,
%where
%we argue that the usage of WaveNet is fundamentally trying to solve two problems at once. 
%Solutions to 
%There may be benefit to observing these problems individually.
%the fundamentally overcomplicating the problem of vocoding spectrograms.
%inverting perceptually-informed spectrograms.

%both of these methods are fundamentally addressing the wrong aspect of the inversion problem. 

%While customized inference kernels can be used to expedite autoregressive vocoding~\cite{arik2017deep}, this approach is still an order of magnitude slower than alternative vocoding strategies which produce lower-fidelity results~\cite{ping2017deep}. 
%Knowledge distillation~\cite{hinton2015distilling} can also be used to transfer an autoregressive WaveNet to a model capable of parallel inference~\cite{oord2017parallel}, 
%JULIAN: Why is this a problem? People in this community probably don't care about "end-to-end training" for the sake of elegance, so better to be explicit about why this limitation is significant.
%but this strategy is post-hoc and therefore cannot be trained end-to-end with the rest of the neural TTS system.
%JULIAN: Why is this counterintuitive? Doesn't seem clear to me why one "should" be faster than the other.
%It is counterintuitive that the process of mapping spectrograms into waveforms should be so disproportionately slow compared to the process of mapping language into spectrograms (over $1500$ times slower in our empirical experimentation with open-source codebases).

% On my Titan V, the autoregressive vocoding procedure was producing about 118 audio samples per second, about 186.9x slower than real time (22050Hz). On the same card, the text-to-spectrogram procedure took 3.5 seconds to produce 2500 spectrogram timesteps or 2500*256 = 640000 audio samples = 29.03 seconds. Hence, it is running about 8.3x FASTER than real time. Thus, text-to-spec is 8.3x realtime and spec-to-wave is 0.00535x realtime, hence they are about 8.3/0.00535 ~= 1550x disproportionate.

The need for vocoding arises from the non-invertibility of standard perceptually-informed spectrograms. 
These compact representations
%(e.g.~log-amplitude, mel-scaled spectrograms) 
exclude much of the information in an audio waveform, 
and thus require a predictive model to fill in the missing information needed to synthesize natural-sounding audio. 
Notably, standard spectrogram representations discard phase information resulting from the short-time Fourier transform (STFT), 
and additionally compress the linearly-scaled axis of the STFT magnitude spectrogram into a logarithmically-scaled one. 
This gives rise to two corresponding vocoding subproblems: 
the well-known problem of \emph{phase estimation}, 
and the less-investigated problem of \emph{magnitude estimation}. 

Conventional wisdom suggests that the primary obstacle to vocoding spectrograms is phase estimation. 
However, 
through a comprehensive ablation study outlined in Section~\ref{sec:crowdmos}, 
we find that coupling a solution to \emph{either} subproblem with a good heuristic for the other results in high-quality speech. 
Hence, 
it may be methodologically advantageous for the research community to examine each subproblem independently, 
in addition to investigating methods (e.g.~WaveNet) which examine both.
%rather than using e.g.~WaveNet to solve both jointly.

%we find that coupling solutions to \emph{magnitude estimation}, 
%i.e.,~the inversion of logarithmically-spaced frequency bins into the linearly-spaced STFT bins, 
%are also 
%\textbf{However, we find that the primary obstacle is not phase estimation, but instead \emph{magnitude estimation}, i.e.,~the inversion of logarithmically-spaced frequency bins into the linearly-spaced STFT bins.}\footnote{We justify this claim through a user study in Section~\ref{sec:crowdmos}, but please see \url{https://youtu.be/8VqIwO8m7dI} for a video explanation.}
%Hence, WaveNets are tasked with both magnitude \emph{and} phase estimation, despite only the former problem being of principle concern.

In this paper, 
we seek to address the magnitude estimation subproblem, 
which hitherto has received little attention from the research community. 
We propose a learning-based method which uses generative adversarial networks~\cite{goodfellow2014generative} to learn a stochastic mapping from perceptually-informed spectrograms into simple magnitude spectrograms. 
%, as there may be multiple linear-frequency spectrograms which map to the same log-frequency one. 
We refer to our approach as \emph{adversarial vocoding}. 
By combining our learning-based solution to magnitude estimation with a modern phase estimation heuristic,
%called \emph{Local Weighted Sums} (LWS)~\cite{lws}, 
we show that adversarial vocoding significantly outperforms na\"ive magnitude estimation methods in terms of mean opinion scores (MOS) and benefits from being hundreds of times faster than autoregressive vocoding.

\subsection{Summary of contributions}

\begin{itemize}
    \item Through an ablation study, we measure the perceived effect of inverting the primary sources of compression in audio features. We observe that coupling solutions to either source with a heuristic for the other result in high-quality speech, motivating research on both problems.
    \item We demonstrate that our proposed magnitude estimation method yields significantly higher mean opinion scores than a na\"ive inversion baseline and faster speeds than state-of-the-art vocoding methods.
    \item We show that our method can effectively vocode highly-compressed ($13$:$1$) audio feature representations.
    %, potentially benefiting audio synthesis pipelines operating on such representations.
\end{itemize}

\section{Audio feature preliminaries}

\label{sec:feature}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.\linewidth]{figures/advoc_paper_fig1.pdf}
    \caption{
    Depiction of common audio feature extraction pipelines and corresponding inversion.
    The two estimation problems ablated in Table~\ref{tab:gl} are outlined in a dotted \textcolor{red}{red} line.
    The magnitude estimation subproblem we focus on in our method is shown in a dashed \textcolor{blue}{blue} line.}
    \label{fig:extract_invert}
\end{figure}

The typical process of transforming waveforms into perceptually-informed spectrograms involves several cascading stages.
Here, we describe spectrogram methodology common to two state-of-the-art TTS systems~\cite{ping2017deep,shen2018natural}. A visual representation is shown in Figure~\ref{fig:extract_invert}.

\textbf{Extraction}~~~
The initial stage consists of decomposing waveforms into time and frequency using the STFT. Then,
%under the psychoacoustical assumption that human hearing perception is mostly invariant to phase, 
the phase information is discarded from the complex STFT coefficients leaving only the linear-amplitude magnitude spectrogram. 
The linearly-spaced frequency bins of the resultant spectrogram are then compressed to fewer bins which are equally-spaced on a logarithmic scale (usually the mel scale~\cite{stevens1937scale}). 
Finally, amplitudes of the resultant spectrogram are made logarithmic to conform to human loudness perception, then optionally clipped and normalized.

\textbf{Inversion}~~~
To heuristically invert this procedure (vocode), 
the inverse of each cascading step is applied in reverse.
First, logarithmic amplitudes are converted to linear ones. 
Then,  
an appropriate magnitude spectrogram is estimated from the mel spectrogram.
%---a common procedure is to use the pseudoinverse of the mel basis. 
Finally, appropriate phase information is estimated from the magnitude spectrogram, and the inverse STFT is used to render audio.

Unless otherwise specified, throughout this paper we operate on waveforms sampled at $22050$Hz using Fourier transforms of size $1024$ and a hop size of $256$. 
We compress magnitude spectrograms to $80$ bins equally spaced along the mel scale from $125$Hz to $7600$Hz.
We apply log amplitude scaling and normalize mel spectrograms to have $120$dB dynamic range. 
Precisely recreating this representation is simple in our codebase.\footnote{\url{https://github.com/chrisdonahue/advoc}}

%The typical process of mapping waveforms into perceptually-informed spectrograms involves several stages. 
%The initial stage consists of decomposing waveforms into time and frequency using the short-time Fourier transform (STFT). 
%Then, as an outcome of decades of psychoacoustic observations, cascading stages of lossy compression are applied: notably, phase information is discarded from the STFT, and both amplitude and frequency are transformed from linear scales to logarithmic ones. 
%First, waveforms are decomposed into time and frequency using the short-time Fourier transform (STFT). 
%Then, the phase of the complex STFT coefficients are discarded leaving only the magnitude coefficients, necessitating later estimation of appropriate phase during inversion.
%To improve correspondence to human frequency perception, the frequency axis of the magnitude spectrogram is then further compressed by aggregating the linear-frequency STFT bins into logarithmically-spaced bins (e.g. on the mel scale). 
%Finally, to improve correspondence to human loudness perception, the amplitudes of the remaining coefficients are transformed from a linear scale to a logarithmic one, and optionally clipped and normalized.
%JULIAN: Is this a video you're creating?
%We encourage the reader to see our video example \url{https://youtu.be/8VqIwO8m7dI}, which demonstrates the impact that each stage has on the ability to heuristically invert the representation.
%\textbf{Informally, we observe that the logarithmic rescaling of the frequency axis appears to be the primary obstacle to reliable inversion of perceptual spectrograms, rather than the conventional wisdom that the issues lie in the re-estimation of phase}.

\section{Ablating magnitude and phase estimation's effect on speech naturalness}
\label{sec:magphs}

In this section we seek to measure the perceived impact of predominant sources of information loss in the audio feature extraction pipelines outlined in Section~\ref{sec:feature}. 
We ablate methods for estimating both STFT magnitudes and phases from log-amplitude mel spectrograms. 
A common heuristic for magnitude estimation is to project the mel-scale spectrogram onto the pseudoinverse of the mel basis which was originally used to generate it. 
As a phase estimation baseline, state-of-the-art TTS research~\cite{ping2017deep,shen2018natural} compares to the iterative Griffin-Lim~\cite{griffinlim} strategy with $60$ iterations (assume that we always are referring to $60$ iterations in subsequent mentions of Griffin-Lim). 
% 60.45420265197754 seconds to invert 16 waveforms with griffin lim
% 9.810181379318237 seconds to invert 16 waveforms with LWS
We additionally consider the more-recent LWS~\cite{lws} strategy which, on our CPU, is about six times faster than Griffin-Lim. 



\subsection{Qualitative measurements}
\label{sec:crowdmos}

%\subsection{Evaluation}
%We conduct a comparative evaluation study by crowd sourcing Mean Opinion Score (MOS) Tests on Amazon Mechanical Turk. The general protocol for the all MOS tests conducted in our study is as follows:\\ 
%Raters are presented with audio synthesized using various methods for the same spectrogram representation and asked to rate each of them on a scale of 1 to 5 with 1 point increments. 
%Each test is conducted for 100 randomly selected spectrogram representations - real or neural network generated depending on the experiment. Audio Generations from various methods corresponding to each spectrogram are rated by at-least 8 individual listeners. Note that, the spectrogram representations used for conducting the MOS tests are not used during training.

\begin{table}[t]
\centering
\caption{Ablating the effect of heuristics for magnitude and phase estimation on MOS of speech naturalness with $95$\% confidence intervals.
\textbf{Bolded} entries show that coupling an ideal solution to either subproblem (real data used as a proxy) with the best heuristic for the other yields speech with only $2$--$9$\% lower MOS than the real speech ($p<0.05$).
%When estimating phase for real magnitude spectrograms, LWS produces significantly more natural speech than Griffin-Lim with $60$ iterations. 
}
\footnotesize
\begin{tabular}{llc}
\toprule
Magnitude source & Phase source & MOS \\
% Source & MOS \\
\midrule
Real data & Real data & $4.30 \pm 0.06$ \\
Real data & Griffin-Lim & $3.69 \pm 0.07$ \\
Real data & LWS & $\mathbf{4.09 \pm 0.06}$  \\
Mel pseudoinverse & Real data & $\mathbf{4.04 \pm 0.06}$ \\
Mel pseudoinverse & Griffin-Lim & $2.47 \pm 0.09$ \\
Mel pseudoinverse & LWS & $2.50 \pm 0.09$  \\
%Magnitude so & $4.21 \pm 0.06$ \\
%Real magnitude + GL60 & $3.79 \pm 0.07$ \\
%Real magnitude + LWS & $3.99 \pm 0.07$ \\
%Est. magnitude + GL60 & $2.89 \pm 0.09$ \\
%Est. magnitude + LWS & $2.89 \pm 0.09$ \\
\bottomrule
\end{tabular}
\label{tab:gl}
\end{table}

Using Amazon Mechanical Turk, we conduct a crowd-sourced mean opinion score study to measure the individual impact of these strategies. 
We randomly select $100$ examples from our holdout dataset (outlined in Section~\ref{sec:experiments}). 
We resynthesize these examples using all combinations of the aforementioned magnitude and phase estimation methods. 
We show a random ordering of the different versions of each example to reviewers in a batch (inducing a comparison) and ask them to rate the naturalness of each on a subjective $1$--$5$ scale with $1$ point increments. 
Each batch is reviewed by $8$ different reviewers resulting in $800$ evaluations of strategy. 
We display MOS with $95$\% confidence intervals in Table~\ref{tab:gl}.

From these results, we conclude that an ideal solution to \emph{either} magnitude or phase estimation (represented by using real data in the rows with bolded MOS) can be coupled with a good heuristic to produce high-quality speech. 
While the ground truth speech is still significantly better than the ideal+heuristic reconstructions, 
the MOS for these methods are only $2$-$9$\% worse than the ground truth ($p < 0.05$). 
Given this, we focus on building strategies for the \emph{magnitude estimation} problem, as it is well-suited to modern deep learning methodology (outlined in Section~\ref{sec:methodology}).

As a secondary conclusion, we observe that---for our speech data---using LWS for phase estimation from real spectrograms yielded significantly higher MOS than using Griffin-Lim. 
Given that it is faster, has no hyperparameters, \emph{and} yields significantly higher MOS in our ablation study, we recommend that all TTS research use LWS as a baseline for phase estimation instead of Griffin-Lim. 
Henceforth, all of our experiments that require phase estimation use LWS.


\section{Methodology}
\label{sec:methodology}

Our goal is to invert the mel scaled spectrogram feature representation into time domain waveform representation. As discussed above, \textit{magnitude estimation} and \textit{phase estimation} are the two main steps involved this inversion. We demonstrate in our experiments, that phase estimation methods works very well on the actual magnitude spectrogram of a waveform rather than the estimated magnitude spectrogram. This study provides evidence that the primary obstacle in generating good quality audio using traditional approaches is in the \textit{magnitude estimation} procedure. Traditionally, magnitude estimation is performed by multiplying the mel spectrogram with the approximate inverse of the mel transformation matrix. Since mel spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation is an oversimplification of the magnitude estimation problem. 

To do a more accurate magnitude estimation, we formulate it as a generative modeling problem and propose a Generative Adversarial Network (GAN)\cite{goodfellow2014generative} based solution.
% Traditional heuristic based approaches achieve this goal using a two step process:
% \begin{enumerate}
%     \item Estimating the magnitude spectrogram from the mel spectrogram by multiplying it with with an approximate inverse of the mel transformation matrix.
%     \item Phase information recovery from the estimated magnitude spectrogram using principled phase recovery methods like Griffin Lim\cite{griffinlim} or LWS\cite{lws}. 
% \end{enumerate}
% While this method is computationally very efficient, there is an observable degradation in the audio quality of the recovered waveform. We hypothesize, that the degradation in the quality of the recovered audio can be attributed to the first step of this recovery process. This hypothesis is based on the observation that the phase recovery methods \cite{griffinlim,lws}, work very well on \textit{actual} magnitude spectrograms of waveforms rather than the \textit{estimated} magnitude spectrograms. 
% Since mel - spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation for generating the magnitude spectrogram from mel spectrogram is an oversimplification of the recovery process. To do a more accurate decompression, we formulate this task of recovering magnitude spectrograms from the compressed mel spectrograms as a generative modeling problem and propose a Generative Adversarial Network (GAN)\cite{goodfellow2014generative} based solution.
GANs are generative models that map samples $z$ from a uniform or gaussian distribution $Z$ to samples $y$ from another distribution $Y$ , $G: z\rightarrow y$ \cite{goodfellow2014generative}. In such an \textit{unconditional} GAN setting, there is no control over the mode of the data being generated. In contrast, Conditional Generative Adversarial Networks \cite{cGAN} allow us to direct the data generation process by conditioning the generation on some additional information $x$ and learn a mapping $G: \{x,z\}\rightarrow y$.

\subsection{Objective}
We propose a conditional GAN based model to generate magnitude spectrograms $y$ conditioned on the linearly scaled mel spectrogram representations $x$. The conditional GAN objective is given by:
\begin{align}
    \mathcal{L}_{cGAN}(G,D) = &\mathbb{E}_{x,y}[\log D(x,y)] + \nonumber \\
                 &\mathbb{E}_{x,z}[\log (1-D(x,G(x,z))],\label{cGAN_equation}
\end{align}
where the generator $G$ tries to minimize this objective against an adversary $D$ that tries to maximize it. i.e $G^*  = \arg\min_G \max_D \mathcal{L}_{cGAN}(G,D)$. In other words, the generator tries to generate realistic data that corresponds to the conditioning input $x$ and fools a discriminator which is being trained to discriminate real data from fake and also judge correspondence with the conditional signal. Previous works have shown that it is beneficial to add a secondary component to the generator loss in order to minimize the $L_1$ distance between the generated output $G(x,z)$ and the target $y$. This way, the adversarial component encourages the generator to generate more realistic results,  while the $L_1$ objective ensures the generated output is close to the target.

\begin{align}
    \mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[{||y-G(x,z)||}_1].\label{L1_equation}
\end{align}
Our final objective therefore becomes:
\begin{align}
    G^*  = \arg\min_G\max_D \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G).\label{full_objective}
\end{align}

Since there are multiple valid magnitude spectrograms corresponding to a compressed mel spectrogram representation, it is important to pass the prior $z$ along with the conditioning signal $x$ to avoid learning a deterministic generator. In practice however, past works \cite{Mathieu2016DeepMV,pix2pix} have observed that in such a setting the generator simply learns to ignore $z$. To counter this problem, Isola et al \cite{pix2pix} proposed to add noise in the form of dropout applied on several layers in the generator at both training and test time. We folow the same and introduce stochasticity in the generation phase using dropout.

\subsection{Network Architecture}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/modelvertical.pdf}
    \caption{Adversarial Vocoder Model}
    \label{fig:model}
\end{figure}

Figure \ref{fig:model} shows the high level training setup for adversarial inversion of mel spectrogram representation into the magnitude spectrogram.

\textbf{Generator:} The generator network $G$ takes in as input the linearly scaled mel spectrogram representation $x$ of shape $(n, \mathit{melBins})$ and generates magnitude spectrogram of shape $(n, 513)$. The generator first estimates the magnitude spectrorgram through a linear projection of the mel spectrogram using the approximate inverse of the mel transformation matrix. The estimated magnitude spectrogram goes through a convolution based encoder-decoder architecture with skip connections similar to the U-net model \cite{unet}. Since our problem essentially reduces to an image to image translation problem, we use the pix2pix generator architecture proposed in \cite{pix2pix}. In our preliminary experiments, we did not find using the batch normalization effective for our setup, so we do not include any batch normalization layers in the generator. Additionally, we also train a smaller generator network which contains 4 encoder layers instead of 8 and reduces the number of channels in each of the convolution layers by half. The decoder follows a mirror architecture of the encoder using transposed convolutional layers. We provide skip connections between the $i_{th}$ layer and the $(n-i)_{th}$ layer of the generator where $n$ is the total number of generator layers.

\textbf{Discriminator:} Previous works have explored that training the generator using just $L_1$ or $L_2$ loss produces blurry results in image translation problems \cite{pathakCVPR16context,zhang2016colorful}. While minimizing $L_1$ loss ensures that the high level structure of the generated output matches the target, we need to augment it with an adversarial loss to help maintain the low level details in the generated output.
In \cite{pix2pix} Isola et al proposed a Markovian Discriminator - \textit{PatchGAN}  that tries to classify an \textit{N X N}  patch in an image as real or fake rather than the whole image. This allows the discriminator to focus on low level detail in the image and the $L_1$ loss takes care of the high level similarity between the input and output. We use the same \textit{PatchGAN} discriminator that takes as input the generated or the real magnitude spectrogram concatentated with the estimated magnitude spectrogram along the channel axis. The discriminator runs convolutionally across the input and averages all responses for different patches to get the final output of D.

\subsection{Inverting the generated Magnitude Spectrogram}
We study two heuristic based methods for phase estimation to generate the waveform from the synthesized magnitude spectrogram namely Griffin Lim \cite{griffinlim} and Local Weighted Sums (LWS) \cite{lws}. In our experiments we find LWS \cite{lws} to be significantly faster and better in terms of mean opinon score (MOS) evaluation than Griffin Lim with 60 iterations. 
We therefore use LWS method for this final stage of our audio synthesis pipeline.


% We observed that it is possible to recover the phase information very accurately using from the actual magnitude spectrogram of the waveform. We  
% linear transformation, and recover the phase information from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
% The degradation in quality of the waveform recovered using this approach can be attributed to the magnitude spectrogram 


% We observe that the phase recovery from magnitude magnitude spectrogram is 
% Since mel spectrogram is a lossy compression of the magnitude spectrogram, we argue that it is not not possible to recover the magnitude spectrogram accurately using a simple linear transformation. 
\section{Experiments}
\label{sec:experiments}
We focus our primary empirical study on the publicly available LJ Speech dataset \cite{ljspeech}, which is popularly utilized by researchers for training TTS systems \cite{waveglow,r9y9}. The dataset contains 13,100 short audio clips of a single speaker reading from 7 non fiction books. Clips vary from length 1 to 10 seconds and have a total length of 24 hours.

Audio is processed using the feature extraction process described in section \ref{sec:feature}. We perform STFT with window size 1024 and hop size 256 and discard the phase information to obtain magnitude spectrogram with 513 frequency bins. The magnitude spectrogram is compressed to the mel spectrogram with $melBins$ frequency bins by multiplication with the mel transformation matrix.  
%Magnitude Spectrograms of shape $(256, \mathit{melBins})$ and $(256, 513)$ respectively. 
We train three models for $\mathit{melBins}$ = 20, 40, 80 to study the feasibility of our technique works for higher levels of mel compression. Each of the models is trained for 100,000 mini-batch iterations using a batch size of 8 which corresponds to XX hours of wall clock training time using NVIDIA Titan 1080 GPU. We set the regularization parameter $\lambda = 10$ for our experiments and use Adam Optimizer with learning rate $2e-4$. 

Additionally, we also train a model on the SC09 dataset \cite{donahue2019wavegan} which contains spoken digits ``zero'' through ``nine''. We use this model to study the effectiveness of our vocoding technique for inverting synthetic spectrograms generated by the SpecGAN model \cite{donahue2019wavegan} for the task of unsupervised audio generation.

% \subsection{Comparing Spectral Inversion Mehtods}
% In this section, we study the effectiveness of the two phase estimation methods: Griffin Lim and LWS. Additionally, we also study the effect of log amplitude re-scaling of the spectrogram on the quality of reconstructed audio. 
% From table XX, we can clearly see that LWS method significantly outperforms 60 iterations of Griffin Lim for vociding actual magnitude spectrograms of a waveform while being XX times faster than Griffin Lim.

% Also, the MOS scores for vocoding from estimated magnitude spectrograms are much worse than vocoding from actual magnitude spectrograms. At the same time the difference between the MOS of actual audio and LWS vocoding from magnitude spectrogram is insignificant.  This supports our claim that the primary obstacle in vocoding from perceptually informed spectrograms is not phase estimation, but magnitude estimation.

\subsection{Vocoding LJ Speech Mel Spectrograms}

We conduct comparitive user studies for various vocoding techniques and report the MOS, speed up against real time (\textit{$\times$ RT}) measured on an Nvidia Titan 1080i GPU, and the size of various vocoding models in Mega Bytes \textit{(MB)} in table \ref{tab:realMOS}. 
We conduct two separate user studies using the protocol described in section \ref{sec:crowdmos}, one for vocoding real mel spectrograms of waveforms from our test dataset and one for vocoding Tacotron-2  generated mel spectrograms for TTS conditioned on text from our test dataset. We use the best (to our knowledge) publicly available implementation for Wavenet \cite{r9y9} and NVIDIA's Waveglow implementation \cite{waveglowgithub} for comparison against prior work. 

\begin{table}[htbp]
\centering
\caption{Comparison of various Vocoding methods on linearly scaled mel Spectrogram Representations with 80 mel frequency bins. \textbf{MOS(Real)} contains qualitative evaluation for vocoding real Mel-Spectrograms from our test data set. \textbf{MOS(TTS)} contains qualitative evaluation for vocoding Tacotron-2 generated mel spectrograms conditioned on text from our test data. \textbf{$\mathbf{\times}$ RT} denotes the speed up over real time, higher being faster. \textbf{MB} presents the size of each vocoding Model in Mega Bytes. 
}
%  Tacotron-2 generated spectrograms conditioned on text \textit{MOS(TTS)}. The fourth column \textit{$\times$ RT} denotes the speed up over real time, higher being faster. The column \textit{MB} presents the size of each vocoding Model in Mega Bytes. 
\footnotesize
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccr}
\toprule
Source & MOS (Real) & MOS (TTS) & $\times$ RT & MB \\
\midrule
Real data & $4.16 \pm 0.06$& $4.28 \pm 0.07$  & $1.000$ &   \\
PseudoInv & $2.91 \pm 0.10$ & $2.12 \pm 0.09$ & $8.836$ & $0.2$ \\
Wavenet \cite{oord2016wavenet} & $3.98 \pm 0.07$ & $3.87 \pm 0.07$ & $0.003$ & $95.0$ \\
WaveGlow \cite{waveglow} & $4.09 \pm 0.06$ & $3.89 \pm 0.07$ & $1.229$ & $334.7$ \\
AdVoc & $3.68 \pm 0.07$ & $3.09 \pm 0.07$ & $3.437$ & $52.1$ \\
\bottomrule
\end{tabular}
% }
\label{tab:realMOS}

\end{table}

Our results demonstrate that our adversarial magnitude estimation technique (AdVoc) significantly outperforms magnitude estimation using psuedo-inverse of the mel basis for both real and tacotron generated mel spectrograms. Our method is more than $1000 \times$ faster than autoregressive Wavenet vocoder while being $1.8 \times$ more memory effecient. As compared we WaveGlow we are $2.8 \times$ faster and $6.4 \times$ memory effecient. 

Additionally, we train our models to perform magnitude estimation for higher levels of mel-compression. We train our adversarial model for vocoding mel-spectrograms with $20$, $40$ and $80$ mel-frequency bins. We compare our adversarial magnitude estimation method against magnitude estimation using pseudo-inverse of the mel basis. Our results in table \ref{tab:compression} demonstrate that our model can vocode highly compressed mel-spectrogram representations with relatively less drop in the perceived audio quality as compared to psuedo-inversion baseline. Reducing the number of mel bins by half results is in a statistically insignificant decrease in the the perceived audio quality using adversarial vocoding.

%Our Adversarial Vocoding method significantly outperforms the magnitude estimation using the approximate inverse of the mel-transformation matrix while being more than 3 times faster than real time audio generation. For the wavenet model we use the best (to out knowledge) publicly available online implementation \cite{r9y9}



\begin{table}[htbp]
\centering
\caption{Comparitive study of vocoding methods for different levels of compression of mel spectrogram. }
\footnotesize
\begin{tabular}{lc}
\toprule
Source & MOS \\
\midrule
Real data & $4.05 \pm 0.07$ \\
Mel-20 + PseudoInv & $2.68 \pm 0.10$ \\
Mel-40 + PseudoInv & $2.84 \pm 0.10$ \\
Mel-80 + PseudoInv & $3.25 \pm 0.09$ \\
Mel-20 + AdVoc & $3.75 \pm 0.07$ \\
Mel-40 + AdVoc & $3.79 \pm 0.07$ \\
Mel-80 + AdVoc & $3.86 \pm 0.07$ \\
\bottomrule
\end{tabular}
\label{tab:compression}
\end{table}

% \subsection{TTS experiments using Tacotron-2}
% We conduct a similar user study as above on the synthetic mel spectrograms generated by tacotron 2 for mel spectrograms generated for 100 randomly selected texts from the LJ SPeech dataset using the pretrained Tacotron-2 model. 







% \begin{table}[t]
% \centering
% \caption{Hello}
% \footnotesize
% \begin{tabular}{lc}
% \toprule
% Source & MOS\\
% \midrule
% Real data & $4.28 \pm 0.07$ \\
% PsuedoInv & $2.12 \pm 0.09$ \\
% Wavenet \cite{oord2016wavenet} & $3.87 \pm 0.07$ \\
% Waveglow \cite{waveglow} & $3.89 \pm 0.07$ \\
% AdVoc & $3.09 \pm 0.07$ \\
% \bottomrule
% \end{tabular}
% \label{tab:tacotronMOS}
% \end{table}

\section{Acknowledgements}

The authors would like to thank Bo Li and Miller Puckette for helpful discussions about this work. 
This research was supported by the UC San Diego Chancellorâ€™s Research Excellence Scholarship program.
Thanks to NVIDIA for GPU donations which were used in the preparation of this work.


\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2019 publication,''
%   in \textit{Interspeech 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2019, pp.~100--104.
% \end{thebibliography}

\end{document}


\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2019}

\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\newcommand{\coollink}[1]{\href{https://#1}{\nolinkurl{#1}}}
\usepackage{bm}

%JULIAN: Is this conference double-blind?
\title{Expediting TTS Synthesis with Adversarial Vocoding}
\name{\text{*}Paarth Neekhara$^1$, \text{*}Chris Donahue$^{2}$, Miller Puckette$^2$, Shlomo Dubnov$^2$, Julian McAuley$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$UC San Diego Department of Computer Science\\
  $^2$UC San Diego Department of Music\\
  \text{*} Equal contribution}
\email{pneekhar@eng.ucsd.edu, cdonahue@ucsd.edu}

\begin{document}

\maketitle
% 
\begin{abstract}
Recent approaches in text-to-speech (TTS) synthesis employ neural network strategies to vocode perceptually-informed spectrogram representations directly into listenable waveforms. 
Such vocoding procedures create a computational bottleneck in modern TTS pipelines. 
We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be heuristically vocoded. 
Through a user study, we show that our approach significantly outperforms na\"ive vocoding strategies while being hundreds of times faster than neural network vocoders used in state-of-the-art TTS systems. 
We also show that our method can be used to achieve state-of-the-art results in unsupervised synthesis of individual words of speech.
%Because our approach is independent of the TTS pipeline, it has the potential to benefit numerous applications where fast vocoding of spectrograms is desirable. 
%JULIAN: This example sounds quite abstract. Say that it can be used for audio generation before saying the specifics, otherwise it's not clear what the significance of the example is.
%As an example, we show that our approach can be used to effectively vocode spectrograms generated by a separate GAN, improving the state of the art in unsupervised audio generation.
\end{abstract}

%\noindent\textbf{Index Terms}: text-to-speech, vocoding, generative adversarial networks

\section{Introduction}

Generating natural-sounding speech from text is a well-studied problem with numerous potential applications. 
While past approaches were built on extensive engineering knowledge in the areas of linguistics and speech processing (see \cite{zen2009statistical} for a review), 
recent approaches adopt neural network strategies which learn from data to map linguistic representations into audio waveforms~\cite{arik2017deep,gibiansky2017deep,ping2017deep,wang2017tacotron,shen2018natural}. 
Of these recent systems, 
the best performing~\cite{ping2017deep,shen2018natural} are both comprised of two functional mechanisms which 
(1) map language into \emph{perceptually-informed spectrogram} representations (i.e.,~time-frequency decompositions of audio with logarithmic scaling of both frequency and amplitude), and 
(2) \emph{vocode} the resultant spectrograms into listenable waveforms. 
In such two-step TTS systems, 
using perceptually-informed spectrograms as intermediaries is observed to have empirical benefits over using representations which are simpler to convert to audio~\cite{ping2017deep}. 
Hence, vocoding is central to the success of state-of-the-art TTS systems, and is the focus of this work.
%In this work, we focus our efforts on the vocoding subproblem.

The need for vocoding arises from the non-invertibility of perceptually-informed spectrograms. 
These compact representations
%(e.g.~log-amplitude, mel-scaled spectrograms) 
exclude much of the information in an audio waveform, 
and thus require a predictive model to fill in the missing information needed to synthesize natural-sounding audio. 
Notably, standard spectrogram representations discard phase information resulting from the short-time Fourier transform (STFT), 
and additionally compress the linearly-scaled frequency axis of the STFT magnitude spectrogram into a logarithmically-scaled one. 
This gives rise to two corresponding vocoding subproblems: 
the well-known problem of \emph{phase estimation}, 
and the less-investigated problem of \emph{magnitude estimation}.

% While the aforementioned TTS approaches differ substantially in their methods of mapping language to spectrograms, they both depend on the same method to vocode spectrograms into high-fidelity, natural-sounding waveforms. 
% Namely, they both utilize WaveNets~\cite{oord2016wavenet} conditioned on the synthetic spectrograms to synthesize audio. 
% WaveNet is an autoregressive method which learns to predict individual audio samples given previous audio samples and---in this case---the desired spectrogram. 
% Henceforth we will refer to this practice of using WaveNet to map spectrograms into waveforms as \emph{autoregressive vocoding}. 

% Autoregressive vocoding is problematic as it necessitates running WaveNet once per individual audio sample (e.g. $22050$ times per second), bottlenecking the overall TTS system as the language-to-spectrogram mechanisms are comparatively fast.\footnote{In our empirical experimentation with open-source codebases, the autoregressive vocoding phase was over $1500$ times slower on average than the language to spectrogram phase.} 
% Cumbersome
% %JULIAN: Cumbersome is a somewhat vague word. Are they cumbersome because they're too elaborate? Or because they don't easily interoperate with different techniques? In some sense it doesn't matter how "cumbersome" they are if they already solve the same problem just as well.
% strategies like knowledge distillation~\cite{hinton2015distilling} and customized inference kernels~\cite{arik2017deep} can be used to accelerate WaveNet inference to be faster than real-time. 
% However, 
% we argue that WaveNet is fundamentally being tasked with solving two problems at once, 
% while more efficient solutions to only one of these problems may suffice.
% %JULIAN: Again I don't quite follow. Is the argument mainly that the solution is just more elegant? It would be nice to argue the value of an elegant solution (e.g. interoperability or something) rather than just making the (subjective) statement that your solution is more elegant.
% %and that a solution to only one of these problems may suffice and ultimately be more efficient.

Vocoding methodology in state-of-the-art TTS systems~\cite{ping2017deep,shen2018natural} endeavors to address the joint of these two subproblems, 
i.e.,~to transform perceptually-informed spectrograms directly into waveforms. 
Specifically, both systems use WaveNet~\cite{oord2016wavenet} conditioned on spectrograms. 
This approach is problematic as it necessitates running WaveNet once per individual audio sample (e.g. $22050$ times per second), bottlenecking the overall TTS system as the language-to-spectrogram mechanisms are comparatively fast.\footnote{In our empirical experimentation with open-source codebases, the autoregressive vocoding phase was over $1500$ times slower on average than the language to spectrogram phase.}
Given that joint solutions currently necessitate such computational overhead, 
it may be methodologically advantageous to combine solutions to the individual subproblems.

Before endeavoring to develop individual solutions to magnitude and phase estimation, 
we first wished to discover which (if any) of the two represented a greater obstacle to vocoding. 
To answer this, we conducted a user study examining the effect that common heuristics for each subproblem have on the perceived naturalness of vocoded speech (Table~\ref{tab:gl}).\footnote{Sound examples: \coollink{chrisdonahue.com/advoc_examples}} 
Our study demonstrated that \textbf{combining an ideal solution to \emph{either} magnitude or phase estimation with a good heuristic for the other results in high-quality speech}. 
%We found this observation to be surprising given that conventional wisdom suggests that phase estimation is the primary obstacle to vocoding. 
Hence, 
we can focus our research efforts on \emph{either} subproblem, 
in the hopes of developing methods which are more computationally efficient than existing end-to-end strategies.

% Conventional wisdom suggests that the primary obstacle to vocoding spectrograms is phase estimation. 
% However, 
% through a comprehensive ablation study outlined in Section~\ref{sec:crowdmos}, 
% we find that coupling a solution to \emph{either} subproblem with a good heuristic for the other results in high-quality speech. 
% Hence, 
% it may be methodologically advantageous for the research community to examine each subproblem independently, 
% in addition to investigating methods (e.g.~WaveNet) which examine both.

%the usage of WaveNet is fundamentally trying to solve two problems at once,
%where
%we argue that the usage of WaveNet is fundamentally trying to solve two problems at once. 
%Solutions to 
%There may be benefit to observing these problems individually.
%the fundamentally overcomplicating the problem of vocoding spectrograms.
%inverting perceptually-informed spectrograms.

%both of these methods are fundamentally addressing the wrong aspect of the inversion problem. 

%While customized inference kernels can be used to expedite autoregressive vocoding~\cite{arik2017deep}, this approach is still an order of magnitude slower than alternative vocoding strategies which produce lower-fidelity results~\cite{ping2017deep}. 
%Knowledge distillation~\cite{hinton2015distilling} can also be used to transfer an autoregressive WaveNet to a model capable of parallel inference~\cite{oord2017parallel}, 
%JULIAN: Why is this a problem? People in this community probably don't care about "end-to-end training" for the sake of elegance, so better to be explicit about why this limitation is significant.
%but this strategy is post-hoc and therefore cannot be trained end-to-end with the rest of the neural TTS system.
%JULIAN: Why is this counterintuitive? Doesn't seem clear to me why one "should" be faster than the other.
%It is counterintuitive that the process of mapping spectrograms into waveforms should be so disproportionately slow compared to the process of mapping language into spectrograms (over $1500$ times slower in our empirical experimentation with open-source codebases).

% On my Titan V, the autoregressive vocoding procedure was producing about 118 audio samples per second, about 186.9x slower than real time (22050Hz). On the same card, the text-to-spectrogram procedure took 3.5 seconds to produce 2500 spectrogram timesteps or 2500*256 = 640000 audio samples = 29.03 seconds. Hence, it is running about 8.3x FASTER than real time. Thus, text-to-spec is 8.3x realtime and spec-to-wave is 0.00535x realtime, hence they are about 8.3/0.00535 ~= 1550x disproportionate.

%rather than using e.g.~WaveNet to solve both jointly.
%JULIAN: This paragraph is already much more convincing than the one above which I commented on. You might try and make this (better) argument earlier otherwise the contribution seems too nebulous earlier on. I'd still try to say *why* it's methodologically advantageous.
%JULIAN: To be cynical, unless you're directly comparing to the above methods, I'd save some of the more vague arguments for the related work (or a discussion section) and put it at the *end* of the paper. There's some risk of the introduction coming across as not quite confident/positive enough.
%JULIAN: Lack of any formal related work section is ballsy

%we find that coupling solutions to \emph{magnitude estimation}, 
%i.e.,~the inversion of logarithmically-spaced frequency bins into the linearly-spaced STFT bins, 
%are also 
%\textbf{However, we find that the primary obstacle is not phase estimation, but instead \emph{magnitude estimation}, i.e.,~the inversion of logarithmically-spaced frequency bins into the linearly-spaced STFT bins.}\footnote{We justify this claim through a user study in Section~\ref{sec:crowdmos}, but please see \url{https://youtu.be/8VqIwO8m7dI} for a video explanation.}
%Hence, WaveNets are tasked with both magnitude \emph{and} phase estimation, despite only the former problem being of principle concern.

In this paper, 
we seek to address the magnitude estimation subproblem, 
which is well-suited for modern deep learning methodology. 
%which hitherto has received little attention from the research community. 
We propose a learning-based method which uses generative adversarial networks~\cite{goodfellow2014generative} to learn a stochastic mapping from perceptually-informed spectrograms into simple magnitude spectrograms. 
We combine this magnitude estimation method with a modern phase estimation heuristic, 
referring to this method as \emph{adversarial vocoding}.
%, as there may be multiple linear-frequency spectrograms which map to the same log-frequency one. 
%We refer to our approach as \emph{adversarial vocoding}. 
%By combining our learning-based solution to magnitude estimation with a modern phase estimation heuristic,
%called \emph{Local Weighted Sums} (LWS)~\cite{lws}, 
We show that adversarial vocoding significantly outperforms heuristic vocoding methods in terms of mean opinion scores (MOS) and benefits from being hundreds of times faster than WaveNet-based vocoding.

\subsection{Summary of contributions}

\begin{itemize}
    \item We measure the perceived effect of inverting the primary sources of compression in audio features. We observe that coupling solutions to either source with a heuristic for the other result in high-quality speech.
    \item For both real spectrograms and synthetic ones from TTS systems, we demonstrate that our proposed vocoding method yields significantly higher mean opinion scores
    %MOS
    than a heuristic baseline and faster speeds than state-of-the-art vocoding methods.
    \item We show that our method can effectively vocode highly-compressed ($13$:$1$) audio feature representations.
    \item We show that our method improves the state of the art in unsupervised synthesis of individual words of speech.
\end{itemize}

\section{Audio feature preliminaries}

\label{sec:feature}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.\linewidth]{figures/advoc_paper_fig1.pdf}
    \caption{
    Depiction of stages in common audio feature extraction pipelines and corresponding inversion. 
    The two obstacles to vocoding are (1) estimating linear-frequency magnitude spectra from log-frequency mel spectra (outlined in \textcolor[rgb]{0.29, 0.33, 0.13}{green} dashed line), and (2) estimating phase information from magnitude spectra (outlined in \textcolor{blue}{blue} dotted line).
    We focus on magnitude estimation in this paper, observing that coupling an ideal solution to this subproblem with a phase estimation heuristic can produce high-quality speech (Table~\ref{tab:gl}).}
    \label{fig:extract_invert}
\end{figure}

The typical process of transforming waveforms into perceptually-informed spectrograms involves several cascading stages.
Here, we describe spectrogram methodology common to two state-of-the-art TTS systems~\cite{ping2017deep,shen2018natural}. A visual representation is shown in Figure~\ref{fig:extract_invert}.

\textbf{Extraction}~~~
The initial stage consists of decomposing waveforms into time and frequency using the STFT. Then,
%under the psychoacoustical assumption that human hearing perception is mostly invariant to phase, 
the phase information is discarded from the complex STFT coefficients leaving only the linear-amplitude magnitude spectrogram. 
The linearly-spaced frequency bins of the resultant spectrogram are then compressed to fewer bins which are equally-spaced on a logarithmic scale (usually the mel scale~\cite{stevens1937scale}). 
Finally, amplitudes of the resultant spectrogram are made logarithmic to conform to human loudness perception, then optionally clipped and normalized.

\textbf{Inversion}~~~
To heuristically invert this procedure (vocode), 
the inverse of each cascading step is applied in reverse.
First, logarithmic amplitudes are converted to linear ones. 
Then,  
an appropriate magnitude spectrogram is estimated from the mel spectrogram.
%---a common procedure is to use the pseudoinverse of the mel basis. 
Finally, appropriate phase information is estimated from the magnitude spectrogram, and the inverse STFT is used to render audio.

Unless otherwise specified, throughout this paper we operate on waveforms sampled at $22050$Hz using Fourier transforms of size $1024$ and a hop size of $256$. 
We compress magnitude spectrograms to $80$ bins equally spaced along the mel scale from $125$Hz to $7600$Hz.
We apply log amplitude scaling and normalize mel spectrograms to have $120$dB dynamic range. 
Precisely recreating this representation~\cite{mcfee2019open} is simple in our codebase.\footnote{Code: \coollink{github.com/chrisdonahue/advoc}}

%The typical process of mapping waveforms into perceptually-informed spectrograms involves several stages. 
%The initial stage consists of decomposing waveforms into time and frequency using the short-time Fourier transform (STFT). 
%Then, as an outcome of decades of psychoacoustic observations, cascading stages of lossy compression are applied: notably, phase information is discarded from the STFT, and both amplitude and frequency are transformed from linear scales to logarithmic ones. 
%First, waveforms are decomposed into time and frequency using the short-time Fourier transform (STFT). 
%Then, the phase of the complex STFT coefficients are discarded leaving only the magnitude coefficients, necessitating later estimation of appropriate phase during inversion.
%To improve correspondence to human frequency perception, the frequency axis of the magnitude spectrogram is then further compressed by aggregating the linear-frequency STFT bins into logarithmically-spaced bins (e.g. on the mel scale). 
%Finally, to improve correspondence to human loudness perception, the amplitudes of the remaining coefficients are transformed from a linear scale to a logarithmic one, and optionally clipped and normalized.
%JULIAN: Is this a video you're creating?
%We encourage the reader to see our video example \url{https://youtu.be/8VqIwO8m7dI}, which demonstrates the impact that each stage has on the ability to heuristically invert the representation.
%\textbf{Informally, we observe that the logarithmic rescaling of the frequency axis appears to be the primary obstacle to reliable inversion of perceptual spectrograms, rather than the conventional wisdom that the issues lie in the re-estimation of phase}.

\section{Measuring the effect of magnitude and phase estimation on speech naturalness}
\label{sec:magphs}

The audio feature extraction pipelines outlined in Section~\ref{sec:feature} have two sources of compression: the discarding of phase information and compression of magnitude information. 
Conventional wisdom suggests that the primary obstacle to inverting such features is phase estimation. 
However, 
to the best of our knowledge, 
a systematic evaluation of the individual contributions of magnitude and phase estimation on perceived naturalness of vocoded speech has never been conducted.

To perform such an evaluation, we mix and match methods for estimating both STFT magnitudes and phases from log-amplitude mel spectrograms. 
A common heuristic for magnitude estimation is to project the mel-scale spectrogram onto the pseudoinverse of the mel basis which was originally used to generate it. 
As a phase estimation baseline, state-of-the-art TTS research~\cite{ping2017deep,shen2018natural} compares to the iterative Griffin-Lim~\cite{griffinlim} strategy with $60$ iterations. 
% 60.45420265197754 seconds to invert 16 waveforms with griffin lim
% 9.810181379318237 seconds to invert 16 waveforms with LWS
We additionally consider the more-recent Local Weighted Sums (LWS)~\cite{lws} strategy which, on our CPU, is about six times faster than $60$ iterations of Griffin-Lim.
As a proxy for an ideal solution to either subproblem, 
we also use magnitude and phase information extracted from real data. 

We show human judges the same waveform vocoded by six different magnitude and phase estimation combinations (inducing a comparison) and ask them to rate the naturalness of each on a subjective $1$ to $5$ scale (full user study methodology outlined in Section~\ref{sec:ljspeechexp}). 
Mean opinion scores are shown in Table~\ref{tab:gl}, and we encourage readers to listen to our sound examples linked from the footnote on the first page to help contextualize.

\begin{table}[t]
\centering
\caption{Ablating the effect of heuristics for magnitude and phase estimation on mean opinion score (MOS) of speech naturalness with $95$\% confidence intervals.
\textbf{Bolded} entries show that coupling an ideal solution to either subproblem (real data used as a proxy) with a good heuristic for the other yields speech with only $2$--$9$\% lower
MOS than real speech ($p<0.05$).
%When estimating phase for real magnitude spectrograms, LWS produces significantly more natural speech than Griffin-Lim with $60$ iterations. 
}
\footnotesize
\begin{tabular}{llc}
\toprule
Magnitude est. method & Phase est. method & MOS \\
% Source & MOS \\
\midrule
\emph{Ideal} (real magnitudes) & \emph{Ideal} (real phases) & $4.30 \pm 0.06$ \\
\emph{Ideal} (real magnitudes) & Griffin-Lim w/ $60$ iters & $3.70 \pm 0.07$ \\
\emph{Ideal} (real magnitudes) & Local Weighted Sums & $\mathbf{4.09 \pm 0.06}$  \\
Mel pseudoinverse & \emph{Ideal} (real phases) & $\mathbf{4.04 \pm 0.06}$ \\
Mel pseudoinverse & Griffin-Lim w/ $60$ iters & $2.48 \pm 0.09$ \\
Mel pseudoinverse & Local Weighted Sums & $2.51 \pm 0.09$  \\
%Magnitude so & $4.21 \pm 0.06$ \\
%Real magnitude + GL60 & $3.79 \pm 0.07$ \\
%Real magnitude + LWS & $3.99 \pm 0.07$ \\
%Est. magnitude + GL60 & $2.89 \pm 0.09$ \\
%Est. magnitude + LWS & $2.89 \pm 0.09$ \\
\bottomrule
\end{tabular}
\label{tab:gl}
\end{table}

From these results, we conclude that an ideal solution to \emph{either} magnitude or phase estimation can be coupled with a good heuristic for the other to produce high-quality speech. 
While the ground truth speech is still significantly more natural than that of ideal+heuristic strategies, 
the MOS for these methods are only $2$-$9$\% worse than the ground truth ($p < 0.05$). 
Of these two problems, 
we choose to focus on building strategies for the magnitude estimation problem, 
as it is well-suited to modern deep learning methodology (outlined in Section~\ref{sec:methodology}).

As a secondary conclusion, we observe that---for our speech data---using LWS for phase estimation from real spectrograms yields significantly higher MOS than using Griffin-Lim. 
Given that it is faster \emph{and} yields significantly more natural speech, we recommend that all TTS research use LWS as a phase estimation baseline instead of Griffin-Lim. 
Henceforth, all of our experiments that require phase estimation use LWS.


\section{Adversarial vocoding}
\label{sec:methodology}
% In the previous section, we demonstrate potential in the magnitude estimation subproblem for vocoding mel-spectrograms.
Our goal is to invert a mel spectrogram feature representation into a time domain waveform representation. 
In the previous section, we demonstrated the potential of the magnitude estimation subproblem for achieving this goal in combination with the LWS phase estimation heuristic. 
A common heuristic for magnitude estimation is performed by multiplying the mel spectrogram with the approximate inverse of the mel transformation matrix.
Since the mel spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation is an oversimplification of the magnitude estimation problem.

% CHRIS: Please add a high-level sentence about GANs before the precise definition... e.g. "GANs are generative models which seek to learn latent structure in the distribution of data. They do this by mapping samples \bm{z} ..."

% As discussed above, \textit{magnitude estimation} and \textit{phase estimation} are the two main steps involved this inversion. We demonstrate in our experiments, that phase estimation methods works very well on the actual magnitude spectrogram of a waveform rather than the estimated magnitude spectrogram. This study provides evidence that the primary obstacle in generating good quality audio using traditional approaches is in the \textit{magnitude estimation} procedure.
In order to perform more accurate magnitude estimation, we formulate it as a generative modeling problem and propose a Generative Adversarial Network (GAN) \cite{goodfellow2014generative} based solution.
% Traditional heuristic based approaches achieve this goal using a two step process:
% \begin{enumerate}
%     \item Estimating the magnitude spectrogram from the mel spectrogram by multiplying it with with an approximate inverse of the mel transformation matrix.
%     \item Phase information recovery from the estimated magnitude spectrogram using principled phase recovery methods like Griffin Lim\cite{griffinlim} or LWS\cite{lws}. 
% \end{enumerate}
% While this method is computationally very efficient, there is an observable degradation in the audio quality of the recovered waveform. We hypothesize, that the degradation in the quality of the recovered audio can be attributed to the first step of this recovery process. This hypothesis is based on the observation that the phase recovery methods \cite{griffinlim,lws}, work very well on \textit{actual} magnitude spectrograms of waveforms rather than the \textit{estimated} magnitude spectrograms. 
% Since mel - spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation for generating the magnitude spectrogram from mel spectrogram is an oversimplification of the recovery process. To do a more accurate decompression, we formulate this task of recovering magnitude spectrograms from the compressed mel spectrograms as a generative modeling problem and propose a Generative Adversarial Network (GAN)\cite{goodfellow2014generative} based solution.
%CHRIS: Please use \bm when you're talking about a vector (e.g. \bm{z})
%CHRIS: Changed "Z" to p_Z
GANs are generative models which seek to learn latent structure in the distribution of data. They do this by mapping samples $\bm{z}$ from a uniform or gaussian distribution $p_Z$ to samples $\bm{y}$ from another distribution $Y$, $G: \bm{z}\rightarrow \bm{y}$ \cite{goodfellow2014generative}. 
%CHRIS: Referring to "modes" is  not appropriate here
%In such an \textit{unconditional} GAN setting, the generator is seeking to estimate $P(\mathit{data})$. 
For our purpose, we use a variation of GAN called \textit{conditional} GAN \cite{cGAN} to model the conditional probability distribution of magnitude spectrograms given a mel spectrogram. 
The pix2pix method~\cite{pix2pix} demonstrates that this conditioning information can be a structurally-rich image, extending GANs to learn stochastic mappings from one image domain (spectrogram domain in our case) to another.
We adapt the pix2pix approach for our task.

%Since we are trying to model the conditional probability distribution of magnitude spectrogrms given mel spectrograms, it makes we  $P( \mathit{mag} | \mathit{mel} )$ 
%CHRIS: Please connect this sentence back to our task. Why do we need a conditional GAN? Please make references to the data distribution being modeld. For example, GANs are seeking to learn $p(data)$, while we might use a conditional GAN to model P(mag | mel), as there are many magnitude spectrograms which might map to the same magnitude spectrogram. I might also even drop the pix2pix citation here e.g. "In [pix2pix] they show that this source of conditioning information can be something as rich as an image, hence the GAN is learning an image mapping task..."
%In contrast, Conditional Generative Adversarial Networks \cite{cGAN} allow us to direct the data generation process by conditioning generation on some additional information $x$ and learn a mapping $G: \{x,z\}\rightarrow y$.

% \subsection{Objective}

%CHRIS: I changed "linearly scaled" to "linear-amplitude". Be careful about this distinction because it's not clear if you're talking about amplitudes or frequency
The conditional GAN objective to generate appropriate magnitude spectrograms $\bm{y}$ given mel spectrograms $\bm{x}$ is:
\begin{align}
    \mathcal{L}_{\mathit{cGAN}}(G,D) = &\mathbb{E}_{\bm{x},\bm{y}}[\log D(\bm{x},\bm{y})] + \nonumber \\
                 &\mathbb{E}_{\bm{x},\bm{z}}[\log (1-D(\bm{x},G(\bm{x},\bm{z}))],\label{cGAN_equation}
\end{align}
where the generator $G$ tries to minimize this objective against an adversary $D$ that tries to maximize it. i.e $G^*  = \arg\min_G \max_D \mathcal{L}_{\mathit{cGAN}}(G,D)$. 
In such a conditional GAN setting, 
the generator tries to ``fool'' the discriminator by generating \textit{realistic} magnitude spectrograms that correspond to the conditioning mel spectrogram.  
% the conditioning information $x$ is being fed to both the generator and discriminator. The discriminator is being trained to classify magnitude spectrograms as either real or fake and also judge their correspondence with $x$. The generator on the other hand tries to ``fool'' the discriminator by generating \textit{realistic} magnitude spectrograms that correspond to the conditioning mel spectrograms.
%In other words, the generator tries to generate data that corresponds to the conditioning input $x$ and fools a discriminator which is being trained to discriminate real data from fake and also judge correspondence with the conditional signal.
Previous works \cite{pix2pix,segan}  have shown that it is beneficial to add a secondary component to the generator loss in order to minimize the $L_1$ distance between the generated output $G(x,z)$ and the target $y$.
%JULIAN: Citation?
%CHRIS: Paarth, move the pix2pix citation here.
This way, the adversarial component encourages the generator to generate more realistic results,  while the $L_1$ objective ensures the generated output is close to the target.

\begin{align}
    \mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[{||y-G(x,z)||}_1].\label{L1_equation}
\end{align}
Our final objective therefore becomes:
\begin{align}
%JULIAN: Remember to use \mathit
    G^*  = \arg\min_G\max_D \mathcal{L}_{\mathit{cGAN}}(G,D) + \lambda \mathcal{L}_{L1}(G).\label{full_objective}
\end{align}

%CHRIS: You need a sentence about Lambda. 
Here, $\lambda$ is a hyperparameter which determines the trade-off between the $L_1$ loss and adversarial loss.

% CHRIS: You definitely need to cite pix2pix sooner since our method is *strongly* based on it. Citing it this late suggests that all we're getting from it is the dropout thing
% Since there are multiple valid magnitude spectrograms corresponding to a compressed mel spectrogram representation, it is important to pass the prior $z$ along with the conditioning signal $x$ to avoid learning a deterministic generator. In practice, however, past work \cite{Mathieu2016DeepMV,pix2pix} has observed that in such a setting the generator simply learns to ignore $z$. To counter this problem, Isola \emph{et al.} \cite{pix2pix} proposed to add noise in the form of dropout applied on several layers in the generator at both training and generation time. We follow the same procedure and introduce stochasticity in the generation phase using dropout.

\subsection{Network architecture}

%CHRIS: Please expand on this caption to be a standalone and concise description of the model, for the drive-by caption reader
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/modelvertical.pdf}
    \caption{Adversarial Vocoder Model: The generator performs an image-to-image translation from the estimated magnitude spectrogram to the actual magnitude spectrogram guided by an adversarial loss from the discriminator and the $L_1$ distance between the generated and actual magnitude spectrogram }
    \label{fig:model}
\end{figure}

Figure \ref{fig:model} shows the high level training setup for adversarial inversion of the mel spectrogram representation into the magnitude spectrogram.

% CHRIS: Mention that the pseudoinverse is *fixed* (i.e. not learned as part of the generator's parameters)
\textbf{Generator}~~~The generator network $G$ takes as input the linear-amplitude mel spectrogram representation $x$ of shape $(n, \mathit{melBins})$ and generates a magnitude spectrogram of shape $(n, 513)$. The generator first estimates the magnitude spectrorgram through a fixed (non trainable) linear projection of the mel spectrogram using the approximate inverse of the mel transformation matrix. The estimated magnitude spectrogram goes through a convolution based encoder-decoder architecture with skip connections as in pix2pix~\cite{pix2pix}. 
%Since our task can be thought of as an image to image translation problem between the estimated magnitude spectrogram and the actual magnitude spectrogram, we use the pix2pix \cite{pix2pix} generator architecture.
% CHRIS: What does rectifying mean here? Also I don't like "reduces". Say something like "Since our problem can be thought of as the task of mapping an \emph{estimated} magnitude spectrogram to an \emph{actual} magnitude spectrogram..."
%Since our problem reduces to rectifying an estimated magnitude spectrogram given the actual magnitude spectrogram as the target, we use the pix2pix generator architecture proposed in \cite{pix2pix}. 
%In our preliminary experiments, we did not find the use of batch normalization effective for our setup, so we do not include any batch normalization layers in the generator.
% CHRIS: Two problems here. One, you've never described the architecture of the original model so it's weird to have number so flayers for the smaller model. Two, you've never introduced the "advoc" shortening. I would punt many of these details to the codebase ("We ommitted specifics of the architecture for brevity, however we point to our codebase for precise model implementations...")
Past works~\cite{Mathieu2016DeepMV,pix2pix} have noted that generators similar to our own empirically learn to ignore latent codes leading to deterministic models. 
We adopt the same policy of using dropout at both training and test time to force the model to be stochastic (as our task is not a one-to-one mapping). 
Additionally, we also train a smaller generator \textit{(Advoc - small)} with fewer convolutional layers and fewer convolutional channels. 
We omit the specifics of our architecture for brevity, however we point to our codebase (link in footnote of previous page) for precise model implementations.
% CHRIS: You've already mention there are skip connections. Strike this.

\textbf{Discriminator}~~~Previous works have
%explored 
found
that training generators similar to our own using just an $L_1$ or $L_2$ loss produces images with reasonable global structure (spatial relationships preserved) but poor local structure (blurry)~\cite{pathakCVPR16context,zhang2016colorful}.
As in~\cite{pix2pix}, we combine an $L_1$ loss with a discriminator which operates on \emph{patches} (subregions) of a spectrogram to help improve the ``sharpness'' of the output.
Our discriminator takes as input the estimated spectrogram and \emph{either} the generated or real magnitude spectrogram. 
Thus, in order to satisfy the discriminator, the generator needs to produce magnitude spectrograms that both correspond to the mel spectrogram \emph{and} look realistic.
%Noting this, Isola \emph{et al}.~\cite{pix2pix} combine an $L_1$ objective with an adversarial objective from a discriminator operating on \emph{patches} of the image.
%As in~\cite{pix2pix}, we adopt a discriminator which operates on \emph{patches} of the spectrogram
%While minimizing an $L_1$ loss ensures that the high level structure of the generated output matches the target, we need to augment it with an adversarial loss to help maintain the low level details in the generated output.
%In \cite{pix2pix} Isola \emph{et al.} proposed a Markovian Discriminator -- \textit{PatchGAN}  that tries to classify an $N \times N$  patch in an image as real or fake rather than the whole image allowing the discriminator to focus on low level details in an image.  
%This allows the discriminator to focus on low-level detail in the image and the $L_1$ loss handles the high level similarity between the input and output. 
% CHRIS: "takes as input the estimated spectrogram \emph{and} either the generated or the real magnitude spectrogram". You can leave out that they are concatenated if it's too wordy.
%We adopt a discriminator that takes as input the estimated spectrogram \emph{and} either the generated or the real magnitude spectrogram.
% CHRIS: "runs convolutionally" is imprecise, also you are personifying it. It is something we are doing, not the discriminator of its own free will. "We average the discriminator activations across all patches to get...."
%The discriminator activations across all patches are averaged to get the final output of $D$.

%CHRIS: I've already mentioned this at least once in my writing (end of section 3). This paragraph should be condensed to a single sentence or perhaps excluded entirely if we really need space.
To complete our adversarial vocoding pipeline, we combine generated magnitude spectrograms with LWS-estimated phase spectrograms and use the inverse STFT to synthesize audio.
%we perform phase estimation on the generated magnitude spectrogram using the LWS method as the final step of our adversarial vocoding pipeline.


% \subsection{Inverting the generated Magnitude Spectrogram}

% We study two heuristic based methods for phase estimation to generate the waveform from the synthesized magnitude spectrogram namely Griffin Lim \cite{griffinlim} and Local Weighted Sums (LWS) \cite{lws}. In our experiments we find LWS \cite{lws} to be significantly faster and better in terms of mean opinon score (MOS) evaluation than Griffin Lim with 60 iterations. 
% We therefore use LWS method for this final stage of our audio synthesis pipeline.


% We observed that it is possible to recover the phase information very accurately using from the actual magnitude spectrogram of the waveform. We  
% linear transformation, and recover the phase information from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
% The degradation in quality of the waveform recovered using this approach can be attributed to the magnitude spectrogram 


% We observe that the phase recovery from magnitude magnitude spectrogram is 
% Since mel spectrogram is a lossy compression of the magnitude spectrogram, we argue that it is not not possible to recover the magnitude spectrogram accurately using a simple linear transformation. 
\section{Experiments}
\label{sec:experiments}
We focus our primary empirical study on the publicly available LJ Speech dataset \cite{ljspeech}, which is popularly used by researchers for training TTS systems \cite{waveglow,r9y9}. 
The dataset contains 13,100 short audio clips of a single female speaker reading from 7 non-fiction books. Clips vary in length from 1 to 10 seconds and have a total length of 24 hours.

Audio is processed using the feature extraction process described in Section \ref{sec:feature}. 
%We perform STFT with window size 1024 and hop size 256 and discard phase information to obtain magnitude spectrograms with 513 frequency bins. The magnitude spectrogram is compressed to a mel spectrogram with
%JULIAN: Please use \mathit
%$\mathit{melBins}$ frequency bins by multiplication with the mel transformation matrix.  
%Magnitude Spectrograms of shape $(256, \mathit{melBins})$ and $(256, 513)$ respectively. 
We train three models for $\mathit{melBins} \in \lbrace 20, 40, 80 \rbrace$ to study the feasibility of our technique 
%works 
for 
%higher 
varying
%CHRIS: Was the batch size really only eight? That's unusually small. Why did you use such small batches?
%PAARTH: Since we are using 256 time-steps in the specgtrogram (~4 seconds)  as opposed to 64, I thought we can get away with a smaller batch size. Other models were trained with batch size 16 or 32 though. But this one was 8 because intially I observed learning was faster with a smaller batch size.
levels of mel compression. Each of the models is trained for 100,000 mini-batch iterations using a batch size of 8 which corresponds to
%JULIAN: Make sure your latex code generates an error so you don't miss things like this
% CHRIS: PLEASE FILL IN THIS WALL CLOCK NUMBER
XX hours of wall clock training time using a NVIDIA 1080Ti GPU. We set the regularization parameter $\lambda = 10$ and use the Adam optimizer~\cite{kingma2014adam} ($\alpha = 0.0002$).

\subsection{Vocoding LJ Speech mel spectrograms}
\label{sec:ljspeechexp}
In this study we are concerned with vocoding both real mel spectrograms extracted from the LJ Speech dataset
\emph{and} mel spectrograms generated by a language-to-spectrogram model~\cite{shen2018natural} trained on LJ Speech. 
We compare both our large (\emph{AdVoc}) and small (\emph{AdVoc-small}) adversarial vocoder models to 
the mel pseudoinverse magnitude estimation heuristic combined with LWS (\emph{Pseudoinv}),
a \emph{WaveNet} vocoder~\cite{shen2018natural}, 
and the recent \emph{WaveGlow}~\cite{waveglow} method.

We randomly select $100$ examples from the holdout dataset of LJ Speech and convert them to mel spectrograms. 
We also synthesize mel spectrograms for each transcript of these same examples using the language-to-spectrogram module from Tacotron 2~\cite{shen2018natural}.
We vocode both the real and synthetic spectrograms to audio using the five methods outlined in the previous paragraph. 
Audio from each method can be found in our sound examples (footnote of first page).

To gauge the relative quality of our methods against others,
we conduct two mean opinion score (comparative) studies with human judges on Amazon Mechanical Turk. 
In the first user study, 
we show each judge a randomly-ordered batch of six versions of the same utterance: the original utterance and the spectrogram of that utterance vocoded by the five aforementioned methods. 
In the second user study, 
we show each judge a batch consisting of the real utterance and five vocodings of a synthetic spectrogram with the same transcript.
Judges are asked to rate the naturalness of each on a subjective $1$--$5$ scale with $1$ point increments. 
Each batch is reviewed by $8$ different reviewers resulting in $800$ evaluations of each strategy.
We display 
%MOS with $95$\% confidence intervals 
mean opinion scores
in Table~\ref{tab:gl}. 
We also include the speed of each method (relative to real time) as measured on GPU, and the sizes of each model's parameters in megabytes. 

% TODO(camera ready): Use Brian's convex optimization in librosa thing instead of pinv
% TODO(camera ready): Make sure this does not produce negative values for magnitude estimation like pinv does...

\begin{table}[t]
\centering
\caption{Comparison of vocoding methods on mel spectrograms with $80$ bins. We display comparative mean opinion scores from two separate user studies for vocoding spectrograms extracted from real speech (MOS-Real) and spectrograms generated by a state-of-the-art TTS method (MOS-TTS) with $95$\% confidence intervals. $\mathbf{\times}$ RT denotes the speed up over real time; higher is faster. MB denotes the size of each model in megabytes.
}
%  Tacotron-2 generated spectrograms conditioned on text \textit{MOS(TTS)}. The fourth column \textit{$\times$ RT} denotes the speed up over real time, higher being faster. The column \textit{MB} presents the size of each vocoding Model in Mega Bytes. 
\footnotesize
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccr}
\toprule
Source & MOS-Real & MOS-TTS & $\times$ RT & MB \\
\midrule
Real data & $4.16 \pm 0.06$& $4.28 \pm 0.07$  & $1.000$ &   \\
Pseudoinverse & $2.91 \pm 0.10$ & $2.12 \pm 0.09$ & $8.836$ & $0.2$ \\
WaveNet \cite{oord2016wavenet} & $3.98 \pm 0.07$ & $3.87 \pm 0.07$ & $0.003$ & $95.0$ \\
WaveGlow \cite{waveglow} & $4.09 \pm 0.06$ & $3.89 \pm 0.07$ & $1.229$ & $334.7$ \\
AdVoc & $3.78 \pm 0.07$ & $2.91 \pm 0.08$ & $3.111$ & $207.7$ \\
AdVoc-small & $3.68 \pm 0.07$ & $3.09 \pm 0.07$ & $3.437$ & $52.0$ \\
\bottomrule
\end{tabular}
% }
\label{tab:realMOS}

\end{table}

Our results demonstrate that---for both real \emph{and} synthetic spectrograms---our adversarial magnitude estimation technique (AdVoc) significantly outperforms magnitude estimation using the psuedoinverse of the mel basis.
%JULIAN: Keep capitalization consistent.
%JULIAN: Again this reads like a tonne of undefined acronyms (to me at least). Very unclear how much of this lingo can be taken as assumed knowledge.
%Tacotron generated mel spectrograms. 
Our method is more than $1000 \times$ faster than the autoregressive WaveNet vocoder while using $1.8 \times$ fewer parameters. 
As compared with WaveGlow we are $2.8 \times$ faster and $6.4 \times$ fewer parameters. 

\begin{table}[t]
\centering
\caption{Comparison of heuristic and adversarial vocoding of spectrograms with different levels of mel compression. Adversarial vocoding can vocode highly compressed mel spectrograms with relatively less drop in speech naturalness compared to a heuristic.}
\footnotesize
\begin{tabular}{lcc}
\toprule
Source & $melBins$ & MOS \\
\midrule
Real data & & $4.05 \pm 0.07$ \\
Pseudoinverse & $20$ & $2.68 \pm 0.10$ \\
Pseudoinverse & $40$ & $2.84 \pm 0.10$ \\
Pseudoinverse & $80$ & $3.25 \pm 0.09$ \\
AdVoc & $20$ & $3.75 \pm 0.07$ \\
AdVoc & $40$ & $3.79 \pm 0.07$ \\
AdVoc & $80$ & $3.86 \pm 0.07$ \\
\bottomrule
\end{tabular}
\label{tab:compression}
\end{table}

Additionally, we train our models to perform magnitude estimation on representations with higher compression. 
Specifically, we train our model to vocode mel spectrograms with $20$, $40$ and $80$ bins. 
We compare our adversarial magnitude estimation method against magnitude estimation using the pseudoinverse of the mel basis. 
We conduct a comparative user study using the same methodology as previously outlined. 
Our results in 
%JULIAN: Note "Table" "Figure", "Section" etc. should be capitalized. Switch to \cref.
Table~\ref{tab:compression} demonstrate that our model can vocode highly compressed mel spectrogram representations with relatively little drop in the perceived audio quality as compared to the psuedoinversion baseline (audio examples in footnote of first page).
% CHRIS: I dropped the weird thing about statistically-insignificant drop in performance


% \subsection{TTS experiments using Tacotron-2}
% We conduct a similar user study as above on the synthetic mel spectrograms generated by tacotron 2 for mel spectrograms generated for 100 randomly selected texts from the LJ SPeech dataset using the pretrained Tacotron-2 model. 

\subsection{Unsupervised audio synthesis}

\begingroup
\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\centering
\caption{Combining our adversarial vocoding approach with GAN-generated mel spectrograms outperforms our prior work in unsupervised generation of individual words by all metrics.}
\vspace{2mm}
\footnotesize
\begin{tabular}{lccc}
\toprule
Source & Inception score & Acc. & MOS  \\
\midrule
% Real (train) & $9.18 \pm 0.04$  \\
Real data & $8.01 \pm 0.24$ & $0.95$ & $3.9 \pm 0.15$ \\
% \midrule
WaveGAN \cite{donahue2019wavegan} & $4.67 \pm 0.01$ & $0.58$ & $2.3 \pm 0.18$   \\
SpecGAN \cite{donahue2019wavegan} & $6.03 \pm 0.04$ & $0.66$ & $1.9 \pm 0.17$ \\
% \midrule
MelSpecGAN + AdVoc & $6.63 \pm 0.03$ & $0.71$ & $3.4 \pm 0.20$ \\
\bottomrule
\end{tabular} 
\label{tab:sc09Small}
% \vspace{-2mm}
\end{table}
\endgroup

In this section we are concerned with the \emph{unsupervised} generation of speech (as opposed to supervised generation in the case of TTS). 
We focus on the SC09 digit generation task proposed in our previous work~\cite{donahue2019wavegan}, 
where the goal is to learn to generate examples of spoken digits ``zero'' through ``nine'' \emph{without} labels. 
We first train a GAN to generate mel spectrograms of spoken digits, then train an adversarial vocoder to generate audio conditioned on those spectrograms. 
Using a pretrained digit classifier, 
we calculate an Inception score~\cite{salimans2016improved} for our approach, finding it to outperform our previous state-of-the-art results by $9$\%. 
We also calculate an ``accuracy'' by comparing human labelings to classifier labels for our generated digits, and also solicit subjective speech quality ratings from listeners, finding that our adversarial vocoding-based method outperforms our previous results (Table~\ref{tab:sc09Small}).

\section{Conclusion}

In this work we have shown that solutions to \emph{either} the magnitude or phase estimation subproblems within common vocoding pipelines can result in high-quality speech. 
We have demonstrated a learning-based method for magnitude estimation which significantly improves upon common heuristics for this task.
We demonstrate that our method can integrate with an existing TTS pipeline to provide comparatively fast waveform synthesis. 
Additionally, our method has advanced the state-of-the-art in unsupervised small-vocabulary speech generation.
%Our method has applications in several domains for which fast vocoding of spectrograms is desirable, as we have demonstrated for both TTS and unsupervised audio generation. 
%We hope that our work inspires more research on the two primary subproblems in vocoding pipelines, as improvements on one subproblem could potentially be combined with improvements on the other.

%\section{Acknowledgements}

%The authors would like to thank Bo Li and Miller Puckette for helpful discussions about this work. 
%This research was supported by the UC San Diego Chancellors Research Excellence Scholarship program.
%Thanks to NVIDIA for GPU donations which were used in the preparation of this work.

\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2019 publication,''
%   in \textit{Interspeech 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2019, pp.~100--104.
% \end{thebibliography}

\end{document}


% \begingroup
% \setlength{\tabcolsep}{2.5pt}
% \begin{table}[t!]
% \centering
% \caption{TODO}
% \vspace{2mm}
% \footnotesize
% \begin{tabular}{l|c|cccc}
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{\emph{Quantitative}} & \multicolumn{4}{c}{\emph{Qualitative (human judges)}} \\
% \toprule
% Experiment & Incpt Score &  Acc. & Quality & Ease & Diversity \\
% \midrule
% Real (train) & $9.18 \pm 0.04$  \\
% Real (test) & $8.01 \pm 0.24$ & $0.95$ & $3.9 \pm 0.1$ & $3.9 \pm 0.2$ & $3.5 \pm 0.2$ \\
% \midrule
% WaveGAN (best) \cite{donahue2019wavegan} & $4.67 \pm 0.01$ & $0.58$ & $2.3 \pm 0.2$ & $2.8 \pm 0.2$ & $3.2 \pm 0.1$ \\
% SpecGAN (best) \cite{donahue2019wavegan} & $6.03 \pm 0.04$ & $0.66$ & $1.9 \pm 0.2$ & $2.8 \pm 0.2$ & $2.6 \pm 0.2$ \\
% \midrule
% MelSpecGAN + Advoc& $6.63 \pm 0.03$ & $0.71$ & $3.4 \pm 0.2$ & $3.0 \pm 0.2$ & $2.8 \pm 0.2$ \\
% \bottomrule
% \end{tabular} 
% \label{tab:sc09Complete}
% % \vspace{-2mm}
% \end{table}
% \endgroup

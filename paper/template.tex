\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2019}

\usepackage{url}

%JULIAN: Is this conference double-blind?
\title{Expediting TTS Synthesis with Adversarial Vocoding}
\name{\text{*}Paarth Neekhara$^1$, \text{*}Chris Donahue$^{2}$, Miller Puckette$^2$, Shlomo Dubnov$^2$, Julian McAuley$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$UC San Diego Department of Computer Science\\
  $^2$UC San Diego Department of Music\\
  \text{*} Equal contribution}
\email{pneekhar@eng.ucsd.edu, cdonahue@ucsd.edu}

\begin{document}

\maketitle
% 
\begin{abstract}
%JULIAN: Is this first sentence straightforward enough to a typical reader of this conference?
Recent approaches to text-to-speech (TTS) synthesis employ complex strategies to \emph{vocode} perceptually-informed spectrogram representations into listenable waveforms. 
%JULIAN: Does it just restrict real-time scenarios or is it really that restrictive? "realistic" is a bit vague here.
%Such an approach is computationally prohibitive in realistic TTS scenarios as inference with WaveNet fundamentally restricts parallelization. 
We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be vocoded to audio using
%JULIAN: Somewhat subjective term
principled strategies. 
Through a user study, we show that our approach is competitive with WaveNet-based vocoding methods for both real spectrograms and synthetic spectrograms generated by a TTS system, while being several orders of magnitude faster. 
Because our approach is independent of the TTS pipeline, it has the potential to benefit a number of applications where fast vocoding of spectrograms is desirable. 
%JULIAN: This example sounds quite abstract. Say that it can be used for audio generation before saying the specifics, otherwise it's not clear what the significance of the example is.
As an example, we show that our approach can be used to effectively vocode spectrograms generated by a separate GAN, improving the state of the art in unsupervised audio generation.
\end{abstract}

%\noindent\textbf{Index Terms}: text-to-speech, vocoding, generative adversarial networks

\begin{figure*}[htp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/model.pdf}
    \caption{Adversarial Vocoder Model}
    \label{fig:model}
\end{figure*}


\section{Introduction}

Generating natural-sounding speech from text is a well-studied problem with numerous potential applications. 
While past approaches were built on extensive engineering knowledge in the areas of linguistics and speech processing  (see %JULIAN: Better to add a placeholder name here so that it actually generates an error. Easy to miss otherwise.
\cite{todo} for a review), 
recent approaches adopt neural network strategies which learn from data to map linguistic representations into audio waveforms~\cite{arik2017deep,gibiansky2017deep,ping2017deep,wang2017tacotron,shen2018natural}. 
Of these recent systems, 
the best performing~\cite{ping2017deep,shen2018natural} are both comprised of two functional and independent mechanisms which 
(1) map language into \emph{perceptually-informed spectrogram} representations (i.e.,~time-frequency decompositions of audio with logarithmic scaling of both frequency and amplitude), and 
(2) \emph{vocode} the resultant spectrograms into listenable waveforms. 
In this work, we focus our efforts on the vocoding subproblem.

%While lossy, TTS systems have been observed to benefit from operating in such domains.
%In~\cite{ping2017deep}, TTS systems pairing perceptual spectrogram generation with autoregressive vocoding were found to substantially outperform systems which operated in heuristically-invertible spectrogram domains.
%Hence, autoregressive vocoding is central to the success of state-of-the-art TTS systems.
%This finding underlines a need for faster strategies for vocoding perceptually-informed spectrograms to audio which do not compromise on audio quality.

While the aforementioned TTS approaches differ substantially in their methods of mapping language to spectrograms, they both depend on the same method to vocode spectrograms into high-fidelity, natural-sounding waveforms. 
Namely, they both utilize WaveNets~\cite{oord2016wavenet} conditioned on the synthetic spectrograms to synthesize audio. 
WaveNet is an autoregressive method which learns to predict individual audio samples given previous audio samples and---in this case---the desired spectrogram. 
Henceforth we will refer to this practice of using WaveNet to map spectrograms into waveforms as \emph{autoregressive vocoding}. 

Autoregressive vocoding is problematic as it necessitates running WaveNet once per individual audio sample (e.g. $16000$ times per second of audio), bottlenecking the overall TTS system as the language-to-spectrogram mechanisms are comparatively fast.\footnote{In our empirical experimentation with open-source codebases, the autoregressive vocoding phase was over $1500$ times slower than the language to spectrogram phase.} 
While 
%cumbersome
strategies like knowledge distillation~\cite{hinton2015distilling} and customized inference kernels~\cite{arik2017deep} can be used to accelerate WaveNet inference to be faster than real-time, 
we argue that the usage of WaveNet is fundamentally overcomplicating the problem of vocoding spectrograms.
%inverting perceptually-informed spectrograms.

%both of these methods are fundamentally addressing the wrong aspect of the inversion problem. 

%While customized inference kernels can be used to expedite autoregressive vocoding~\cite{arik2017deep}, this approach is still an order of magnitude slower than alternative vocoding strategies which produce lower-fidelity results~\cite{ping2017deep}. 
%Knowledge distillation~\cite{hinton2015distilling} can also be used to transfer an autoregressive WaveNet to a model capable of parallel inference~\cite{oord2017parallel}, 
%JULIAN: Why is this a problem? People in this community probably don't care about "end-to-end training" for the sake of elegance, so better to be explicit about why this limitation is significant.
%but this strategy is post-hoc and therefore cannot be trained end-to-end with the rest of the neural TTS system.
%JULIAN: Why is this counterintuitive? Doesn't seem clear to me why one "should" be faster than the other.
%It is counterintuitive that the process of mapping spectrograms into waveforms should be so disproportionately slow compared to the process of mapping language into spectrograms (over $1500$ times slower in our empirical experimentation with open-source codebases).

% On my Titan V, the autoregressive vocoding procedure was producing about 118 audio samples per second, about 186.9x slower than real time (22050Hz). On the same card, the text-to-spectrogram procedure took 3.5 seconds to produce 2500 spectrogram timesteps or 2500*256 = 640000 audio samples = 29.03 seconds. Hence, it is running about 8.3x FASTER than real time. Thus, text-to-spec is 8.3x realtime and spec-to-wave is 0.00535x realtime, hence they are about 8.3/0.00535 ~= 1550x disproportionate.

The need for vocoding arises from the non-invertibility of standard perceptually-informed spectrograms. 
These compact representations
%(e.g.~log-amplitude, mel-scaled spectrograms) 
exclude much of the information in an audio waveform, 
and thus require a predictive model to fill in the missing information needed to synthesize natural-sounding audio. 
Notably, standard spectrogram representations discard phase information resulting from the short-time Fourier transform (STFT), 
and additionally compress the linearly-scaled axis of the STFT magnitude spectrogram into a logarithmically-scaled one. 
Conventional wisdom suggests that the primary obstacle to vocoding spectrograms is the need to estimate suitable phase information from the magnitude spectrogram. 
\textbf{However, we find that the primary obstacle is not phase estimation but instead the inversion of logarithmically-scaled frequency axes into the linearly-scaled STFT axis.}\footnote{We later justify this claim through a user study, but please see \url{https://youtu.be/8VqIwO8m7dI} for a video explanation.}
Hence, WaveNet is attempting to both invert axes \emph{and} estimate phase, despite only the former problem being of principle concern.

In this paper, we propose a simple method to map spectrograms with logarithmically-scaled frequency axes into linearly-scaled STFT magnitude spectrograms, i.e.,~to address what we observe to be the primary source of distortion. 
Our method uses generative adversarial networks~\cite{goodfellow2014generative} to learn a stochastic mapping, as there may be multiple linear-frequency spectrograms which map to the same log-frequency one. 
We refer to our approach as \emph{adversarial vocoding}. 
By combining our learning-based solution to this single source of compression with a modern phase estimation heuristic called \emph{Local Weighted Sums} (LWS)~\cite{lws}, 
we show that adversarial vocoding is competitive with autoregressive vocoding in mean opinion scores (MOS) and benefits from being hundreds of times faster.

\subsection{Summary of contributions}

\begin{itemize}
    % Table 2
    \item We demonstrate that our method of mapping perceptually-informed spectrograms to STFT magnitude spectrograms and using heuristics to estimate phase yields significantly higher mean opinion scores than a na\"ive inversion baseline and faster speeds than state-of-the-art vocoding methods.
    % Table 1
    \item We provide definitive evidence that the LWS method~\cite{lws} should be the used as a phase estimation baseline for TTS instead of Griffin-Lim~\cite{griffinlim}. 
    % Table 3
    \item Something about Tacotron i.e. TTS
    % Table 4
    \item Something about GAN?
\end{itemize}

\section{Audio feature extraction preliminaries}

%The typical process of mapping waveforms into perceptually-informed spectrograms involves several stages. 
%The initial stage consists of decomposing waveforms into time and frequency using the short-time Fourier transform (STFT). 
%Then, as an outcome of decades of psychoacoustic observations, cascading stages of lossy compression are applied: notably, phase information is discarded from the STFT, and both amplitude and frequency are transformed from linear scales to logarithmic ones. 
%First, waveforms are decomposed into time and frequency using the short-time Fourier transform (STFT). 
%Then, the phase of the complex STFT coefficients are discarded leaving only the magnitude coefficients, necessitating later estimation of appropriate phase during inversion.
%To improve correspondence to human frequency perception, the frequency axis of the magnitude spectrogram is then further compressed by aggregating the linear-frequency STFT bins into logarithmically-spaced bins (e.g. on the mel scale). 
%Finally, to improve correspondence to human loudness perception, the amplitudes of the remaining coefficients are transformed from a linear scale to a logarithmic one, and optionally clipped and normalized.
%JULIAN: Is this a video you're creating?
%We encourage the reader to see our video example \url{https://youtu.be/8VqIwO8m7dI}, which demonstrates the impact that each stage has on the ability to heuristically invert the representation.
%\textbf{Informally, we observe that the logarithmic rescaling of the frequency axis appears to be the primary obstacle to reliable inversion of perceptual spectrograms, rather than the conventional wisdom that the issues lie in the re-estimation of phase}.

\section{Methodology}
Our goal is to invert the mel scaled spectrogram feature representation into time domain waveform representation. 
Traditional heuristic based approaches achieve this goal using a two step process:
\begin{enumerate}
    \item Estimating the magnitude spectrogram from the mel spectrogram by multiplying it with with an approximate inverse of the mel transformation matrix.
    \item Phase information recovery from the estimated magnitude spectrogram using principled phase recovery methods like Griffin Lim\cite{griffinlim} or LWS\cite{lws}. 
\end{enumerate}

While this method is computationally very efficient, there is an observable degradation in the audio quality of the recovered waveform. We hypothesize, that the degradation in the quality of the recovered audio can be attributed to the first step of this recovery process. This hypothesis is based on the observation that the phase recovery methods \cite{griffinlim,lws}, work very well on \textit{actual} magnitude spectrograms of waveforms rather than the \textit{estimated} magnitude spectrograms. 
Since mel - spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation for generating the magnitude spectrogram from mel spectrogram is an oversimplification of the recovery process. To do a more accurate decompression, we formulate this task of recovering magnitude spectrograms from the compressed mel spectrograms as a generative modeling problem and propose a Generative Adversarial Network (GAN)\cite{goodfellow2014generative} based solution.

GANs are generative models that map samples $z$ from a uniform or gaussian distribution $Z$ to samples $y$ from another distribution $Y$ , $G: z\rightarrow y$ \cite{goodfellow2014generative}. In such an \textit{unconditional} GAN setting, there is no control over the mode of the data being generated. In contrast, Conditional Generative Adversarial Networks \cite{cGAN} allow us to direct the data generation process by conditioning the generation on some additional information $x$ and learn a mapping $G: \{x,z\}\rightarrow y$.

\subsection{Objective}
We propose a conditional GAN based model to generate magnitude spectrograms $y$ conditioned on the compressed mel representations $x$. The conditional GAN objective is given by:
\begin{align}
    \mathcal{L}_{cGAN}(G,D) = &\mathbb{E}_{x,y}[\log D(x,y)] + \nonumber \\
                 &\mathbb{E}_{x,z}[\log (1-D(x,G(x,z))],\label{cGAN_equation}
\end{align}
where the generator $G$ tries to minimize this objective against an adversary $D$ that tries to maximize it. i.e $G^*  = \arg\min_G \max_D \mathcal{L}_{cGAN}(G,D)$. In other words, the generator tries to generate realistic data that corresponds to the conditioning input $x$ and fools a discriminator which is being trained to discriminate real data from fake and also judge correspondence with the conditional signal. Previous works have shown that it is beneficial to add a secondary component to the generator loss in order to minimize the $L_1$ distance between the generated output $G(x,z)$ and the target $y$. This way, the adversarial component encourages the generator to generate more realistic results,  while the $L_1$ objective ensures the generated output is close to the target.

\begin{align}
    \mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[{||y-G(x,z)||}_1].\label{L1_equation}
\end{align}
Our final objective therefore becomes:
\begin{align}
    G^*  = \arg\min_G\max_D \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G).\label{full_objective}
\end{align}

Since there are multiple valid magnitude spectrograms corresponding to a compressed mel spectrogram representation, it is important to pass the prior $z$ along with the conditioning signal $x$ to avoid learning a deterministic generator. In practice however, past works \cite{Mathieu2016DeepMV,pix2pix} have observed that in such a setting the generator simply learns to ignore $z$. To counter this problem, Isola et al \cite{pix2pix} proposed to add noise in the form of dropout applied on several layers in the generator at both training and test time. We folow the same and introduce stochasticity in the generation phase using dropout.

\subsection{Network Architecture}
Figure \ref{fig:model} shows the high level training setup for adversarial inversion of mel spectrogram representation into the magnitude spectrogram.

\textbf{Generator:} The generator network $G$ takes in as input the mel spectrogram representation $x$ of shape $(n, \mathit{melBins})$ and generates magnitude spectrogram of shape $(n, 513)$. The generator first estimates the magnitude spectrorgram through a linear projection of the mel spectrogram using the approximate inverse of the mel transformation matrix. The estimated magnitude spectrogram goes through a convolution based encoder-decoder architecture with skip connections similar to the U-net model \cite{unet}. We use the same network architecture proposed in \cite{pix2pix}. In our preliminary experiments, we did not find using the batch normalization effective for our setup, so we do not include any batch normalization layers in the generator. Additionally, we also train a smaller generator network which contains 4 encoder layers instead of 8 and reduces the number of channels in each of the convolution layers by half. The decoder follows a mirror architecture of the encoder using transposed convolutional layers. We provide skip connections between the $i_{th}$ layer and the $(n-i)_{th}$ layer of the generator where $n$ is the total number of generator layers.

\textbf{Discriminator:} Previous works have explored that training the generator using just $L_1$ or $L_2$ loss produces blurry results in image translation problems \cite{pathakCVPR16context,zhang2016colorful}. While minimizing $L_1$ loss is ensures that the high level structure of the generated output matches the target, we need to augment it with an adversarial loss to help maintain the low level details in the generated output.
In \cite{pix2pix} Isola et al proposed a Markovian Discriminator - \textit{PatchGAN}  that tries to classify an \textit{N X N}  patch in an image as real or fake rather than the whole image. This allows the discriminator to focus on low level detail in the image and the $L_1$ loss takes care of the high level similarity between the input and output. We use the same \textit{PatchGAN} discriminator that takes as input the generated or the real magnitude spectrogram concatentated with the estimated magnitude spectrogram along the channel axis. The discriminator runs convolutionally across the input and averages all responses for different patches to get the final output of D.

\subsection{Inverting the generated Magnitude Spectrogram}
We study two heuristic based methods for phase estimation to generate the waveform from the synthesized magnitude spectrogram namely Griffin Lim \cite{griffinlim} and Local Weighted Sums (LWS) \cite{lws}. In our experiments we find LWS \cite{lws} to be significantly faster and better in terms of mean opinon score (MOS) evaluation than Griffin Lim with 60 iterations. 
We therefore use LWS method for this final stage of our audio synthesis pipeline.


% We observed that it is possible to recover the phase information very accurately using from the actual magnitude spectrogram of the waveform. We  
% linear transformation, and recover the phase information from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
% The degradation in quality of the waveform recovered using this approach can be attributed to the magnitude spectrogram 


% We observe that the phase recovery from magnitude magnitude spectrogram is 
% Since mel spectrogram is a lossy compression of the magnitude spectrogram, we argue that it is not not possible to recover the magnitude spectrogram accurately using a simple linear transformation. 
\section{Experiments}
We focus our primary empirical study on the publicly available LJ Speech dataset \cite{ljspeech}, which is popularly utilized by researchers for training TTS systems \cite{waveglow,r9y9}. The dataset contains 13,100 short audio clips of a single speaker reading from 7 non fiction books. Clips vary from length 1 to 10 seconds and have a total length of 24 hours.

Audio is processed using the feature extraction process described in section XX to obtain pairs of Mel and Magnitude Spectrograms of shape $(256, \mathit{melBins})$ and $(256, 513)$ respectively. We train three models for $\mathit{melBins}$ = 20, 40, 80 to study the feasibility of our technique works for higher levels of mel compression. Each of the models is trained for 100000 mini-batch iterations using a batch size of 8 which corresponds to XX hours of wall clock training time using NVIDIA Titan 1080 GPU. We use Adam Optimizer with step size . 

Additionally, we also train a model on the SC09 dataset \cite{donahue2019wavegan} which contains spoken digits ``zero'' through ``nine''. We use this model to study the effectiveness of our vocoding technique for inverting synthetic spectrograms generated by the SpecGAN model \cite{donahue2019wavegan} for the task of unsupervised audio generation.

\subsection{Evaluation}


\subsubsection{Headings}

Section headings are centered in boldface with the first word capitalized and the rest of the heading in lower case. Sub- headings appear like major headings, except they start at the left margin in the column. Sub-sub-headings appear like sub-headings, except they are in italics and not boldface. See the examples in this file. No more than 3 levels of headings should be used.

\subsection{Equations}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{figure.pdf}
%   \caption{Schematic diagram of speech production.}
%   \label{fig:speech_production}
% \end{figure}

\subsection{Hyperlinks}

For technical reasons, the proceedings editor will strip all active links from the papers during processing. Hyperlinks can be included in your paper, if written in full, e.\,g.\ ``http://www.foo.com/index.html''. The link text must be all black. 
Please make sure that they present no problems in printing to paper.

\subsection{References}

The reference format is the standard IEEE one. References should be numbered in order of appearance, for example \cite{Davis80-COP}, \cite{Rabiner89-ATO}, \cite[pp.\ 417--422]{Hastie09-TEO}, and \cite{YourName17-XXX}.

\section{Acknowledgements}

The authors would like to thank Bo Li and Miller Puckette for helpful discussions about this work. 
This research was supported by the UC San Diego Chancellorâ€™s Research Excellence Scholarship program.
Thanks to NVIDIA for GPU donations which were used in the preparation of this work.


\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2019 publication,''
%   in \textit{Interspeech 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2019, pp.~100--104.
% \end{thebibliography}

\end{document}


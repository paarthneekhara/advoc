\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2019}

\usepackage{url}

\title{Expediting TTS Synthesis with Adversarial Vocoding}
\name{\text{*}Paarth Neekhara$^1$, \text{*}Chris Donahue$^{2}$, Miller Puckette$^2$, Shlomo Dubnov$^2$, Julian McAuley$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$UC San Diego Department of Computer Science\\
  $^2$UC San Diego Department of Music\\
  \text{*} Equal contribution}
\email{pneekhar@eng.ucsd.edu, cdonahue@ucsd.edu}

\begin{document}

\maketitle
% 
\begin{abstract}
Recent approaches to text-to-speech (TTS) synthesis employ WaveNet to \emph{vocode} perceptually-informed spectrogram representations into listenable waveforms. 
Such an approach is computationally prohibitive in realistic TTS scenarios as inference with WaveNet fundamentally restricts parallelization. 
We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be vocoded to audio using principled strategies. 
Through a user study, we show that our approach is competitive with WaveNet-based vocoding methods for both real spectrograms and synthetic spectrograms generated by a TTS system, while being several orders of magnitude faster. 
Because our approach is independent of the TTS pipeline, it has the potential to benefit a number of applications where fast vocoding of spectrograms is desirable. 
As an example, we show that our approach can be used to effectively vocode spectrograms generated by a separate GAN, improving the state of the art in unsupervised audio generation.
\end{abstract}

%\noindent\textbf{Index Terms}: text-to-speech, vocoding, generative adversarial networks

\section{Introduction}

Generating natural-sounding speech from text is a well-studied problem with numerous potential applications. 
While past approaches were built on extensive engineering knowledge in the areas of linguistics and speech processing  (see \cite{} for a review), 
recent approaches adopt neural network strategies which learn from data to map linguistic representations into audio waveforms~\cite{arik2017deep,gibiansky2017deep,ping2017deep,wang2017tacotron,shen2018natural}. 
Of these recent systems, 
the best performing~\cite{ping2017deep,shen2018natural} are both comprised of two functional and independent mechanisms which 
1) map language into \emph{perceptually-informed spectrogram} representations (i.e. spectrograms with logarithmic scaling of both frequency and amplitude), and 
2) \emph{vocode} resultant spectrograms into listenable waveforms. 
In this work, we focus our efforts on the vocoding subproblem.

While the aforementioned TTS approaches differ substantially in their methods of mapping language to spectrograms, they both utilize the same method to vocode spectrograms into high-fidelity, natural-sounding waveforms. 
Specifically, they both utilize WaveNets~\cite{oord2016wavenet} conditioned on the synthetic spectrograms to synthesize audio. 
WaveNet is an autoregressive method which learns to predict individual audio samples given previous audio samples and---in this case---the desired spectrogram. 
Henceforth we will refer to this practice of using WaveNet to map spectrograms into waveforms as \emph{autoregressive vocoding}. 

Autoregressive vocoding is problematic as it necessitates running WaveNet once per individual audio sample (e.g. $16000$ times per second of audio), bottlenecking the overall TTS system as the language-to-spectrogram mechanisms are comparatively fast. 
While customized inference kernels can be used to expedite autoregressive vocoding~\cite{arik2017deep}, this approach is still an order of magnitude slower than alternative vocoding strategies which produce lower-fidelity results~\cite{ping2017deep}. 
Knowledge distillation~\cite{hinton2015distilling} can also be used to transfer an autoregressive WaveNet to a model capable of parallel inference~\cite{oord2017parallel}, 
but this strategy is post-hoc and therefore cannot be trained end-to-end with the rest of the neural TTS system.
It is counterintuitive that the process of mapping spectrograms into waveforms should be so disproportionately slow compared to the process of mapping language into spectrograms (over $1500$ times slower in our empirical experimentation with open-source codebases).

% On my Titan V, the autoregressive vocoding procedure was producing about 118 audio samples per second, about 186.9x slower than real time (22050Hz). On the same card, the text-to-spectrogram procedure took 3.5 seconds to produce 2500 spectrogram timesteps or 2500*256 = 640000 audio samples = 29.03 seconds. Hence, it is running about 8.3x FASTER than real time. Thus, text-to-spec is 8.3x realtime and spec-to-wave is 0.00535x realtime, hence they are about 8.3/0.00535 ~= 1550x disproportionate.

The need for autoregressive vocoding arises primarily from the non-invertibility of standard perceptually-informed spectrograms.
These compact representations (e.g. log-amplitude, mel-scaled spectrograms) discard much of the information in an audio waveform, 
and thus require a predictive model to fill in the missing information needed to synthesize natural-sounding audio.
While lossy, TTS systems have been observed to benefit from operating in such domains.
In~\cite{ping2017deep}, TTS systems pairing perceptual spectrogram generation with autoregressive vocoding were found to substantially outperform systems which operated in heuristically-invertible spectrogram domains.
Hence, autoregressive vocoding is central to the success of state-of-the-art TTS systems.
This finding underlines a need for faster strategies for vocoding perceptually-informed spectrograms to audio which do not compromise on audio quality.

The typical process of mapping waveforms into perceptually-informed spectrograms involves several stages. 
The initial stage consists of decomposing waveforms into time and frequency using the short-time Fourier transform (STFT). 
Then, as an outcome of decades of psychoacoustic observations, cascading stages of lossy compression are applied: notably, phase information is discarded from the STFT, and both amplitude and frequency are transformed from linear scales to logarithmic ones. 
%First, waveforms are decomposed into time and frequency using the short-time Fourier transform (STFT). 
%Then, the phase of the complex STFT coefficients are discarded leaving only the magnitude coefficients, necessitating later estimation of appropriate phase during inversion.
%To improve correspondence to human frequency perception, the frequency axis of the magnitude spectrogram is then further compressed by aggregating the linear-frequency STFT bins into logarithmically-spaced bins (e.g. on the mel scale). 
%Finally, to improve correspondence to human loudness perception, the amplitudes of the remaining coefficients are transformed from a linear scale to a logarithmic one, and optionally clipped and normalized.
We encourage the reader to see this video example \url{FORTHCOMING}, which demonstrates the impact that each stage has on the ability to heuristically invert the representation.
\textbf{Informally, we observe that the logarithmic rescaling of the frequency axis appears to be the primary obstacle to reliable inversion of perceptual spectrograms}.

In this paper, we propose a simple method to map spectrograms with logarithmically-scaled frequency axes into spectrograms with linearly-scaled frequency axes, i.e. to address what we observe to be the primary source of distortion. 
Our method uses generative adversarial networks~\cite{goodfellow2014generative} to learn a stochastic mapping, as there may be multiple linear-frequency spectrograms which map to the same log-scaled one. 
We refer to our approach as \emph{adversarial vocoding}. 
By combining our learning-based solution to this single source of compression with heuristics for the others, 
we show that adversarial vocoding is competitive with autoregressive vocoding in mean opinion scores (MOS) and benefits from being hundreds of times faster.

\subsection{Contributions}

\begin{itemize}
    \item hello
\end{itemize}

\section{Methodology}
Our goal is to invert the mel spectrogram feature representation into time domain waveform representation. 
Traditional heuristic based approaches achieve this goal using a two step process:
1) Estimating the magnitude spectrogram from the mel spectrogram using a linear transformation.
2) Phase information recovery from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
   
While this method is computationally very efficient, there is an \textit{observable degradation in the audio quality??} of the recovered waveform. We hypothesize, that the degradation in the quality of the recovered audio can be attributed to the first step of this recovery process. This hypothesis is based on the observation that the phase recovery methods \cite{lws}, work very well on \textit{actual} magnitude spectrograms of waveforms rather than the \textit{estimated} magnitude spectrograms. 
Since mel - spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation for generating the magnitude spectrogram from mel spectrogram is an oversimplification of the recovery process. To do a more accurate decompression, we formulate this task of recovering magnitude spectrograms from the compressed mel spectrograms as a generative modeling problem and propose a Generative Adversarial Network (GAN)\cite{} based solution.

GANs are generative models that map samples $z$ from a prior distribution $Z$ to samples $y$ from another distribution $Y$ , $G: z\rightarrow y$ \cite{goodfellow2014generative}. In such an \textit{unconditional} GAN setting, there is no control over the mode of the data being generated. In contrast, Conditional Generative Adversarial Networks \cite{condtionalGAN} allow us to direct the data generation process by conditioning the generation on some additional information $x$ and learn a mapping $G: \{x,z\}\rightarrow y$.

\subsection{Objective}
We propose a conditional GAN based model to generate magnitude spectrograms $y$ conditioned on the compressed representations $x$. The conditional GAN objective is given by:
\begin{align}
    \mathcal{L}_{cGAN}(G,D) = &\mathbb{E}_{x,y}[\log D(x,y)] + \nonumber \\
                 &\mathbb{E}_{x,z}[\log (1-D(x,G(x,z))],\label{cGAN_equation}
\end{align}
where the generator $G$ tries to minimize this objective against an adversary $D$ that tries to maximize it. i.e $G^*  = \arg\min_G \max_D \mathcal{L}_{cGAN}(G,D)$. Previous works have shown that it is beneficial to add a secondary component to the generator loss in order to minimize the $L_1$ distance between the generated output $G(x,z)$ and the target $y$. This way, the adversarial component encourages the generator to generate more realistic results,  while the $L_1$ objective ensures the generated output is close to the target.

\begin{align}
    \mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[{||y-G(x,z)||}_1].\label{L1_equation}
\end{align}
Final objective:
\begin{align}
    G^*  = \arg\min_G\max_D \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G).\label{full_objective}
\end{align}
\subsection{Network Architecture}
We formulate the problem as a 
% We observed that it is possible to recover the phase information very accurately using from the actual magnitude spectrogram of the waveform. We  
% linear transformation, and recover the phase information from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
% The degradation in quality of the waveform recovered using this approach can be attributed to the magnitude spectrogram 


% We observe that the phase recovery from magnitude magnitude spectrogram is 
% Since mel spectrogram is a lossy compression of the magnitude spectrogram, we argue that it is not not possible to recover the magnitude spectrogram accurately using a simple linear transformation. 

\subsubsection{Headings}

Section headings are centered in boldface with the first word capitalized and the rest of the heading in lower case. Sub- headings appear like major headings, except they start at the left margin in the column. Sub-sub-headings appear like sub-headings, except they are in italics and not boldface. See the examples in this file. No more than 3 levels of headings should be used.

\subsection{Equations}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{figure.pdf}
%   \caption{Schematic diagram of speech production.}
%   \label{fig:speech_production}
% \end{figure}

\begin{table}[t]
  \caption{Main predefined styles in Word}
  \label{tab:word_styles}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Style Name}      & \textbf{Entities in a Paper}                \\
    \midrule
    Title                    & Title                                       \\
    Author                   & Author name                                 \\
    \textbullet\ List Bullet & Bulleted lists                              \\\relax
    [1] Reference            & References                                  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Hyperlinks}

For technical reasons, the proceedings editor will strip all active links from the papers during processing. Hyperlinks can be included in your paper, if written in full, e.\,g.\ ``http://www.foo.com/index.html''. The link text must be all black. 
Please make sure that they present no problems in printing to paper.

\subsection{References}

The reference format is the standard IEEE one. References should be numbered in order of appearance, for example \cite{Davis80-COP}, \cite{Rabiner89-ATO}, \cite[pp.\ 417--422]{Hastie09-TEO}, and \cite{YourName17-XXX}.

\section{Acknowledgements}

The authors would like to thank Bo Li and Miller Puckette for helpful discussions about this work. 
This research was supported by the UC San Diego Chancellorâ€™s Research Excellence Scholarship program.
Thanks to NVIDIA for GPU donations which were used in the preparation of this work.


\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2019 publication,''
%   in \textit{Interspeech 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2019, pp.~100--104.
% \end{thebibliography}

\end{document}

